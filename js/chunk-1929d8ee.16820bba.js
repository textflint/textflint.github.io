(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-1929d8ee"],{"0312":function(e){e.exports=JSON.parse('[{"name":"WSC273","description":"WSC273 is a common sense reasoning benchmark that requires the system to read a sentence with an ambiguous pronoun and select the referent of that pronoun from two choices. It contains the first 273 examples from the Winograd Schema Challenge. A Winograd schema is a pair of sentences that differ in only one or two words and that contain an ambiguity that is resolved in opposite ways in the two sentences and requires the use of world knowledge and reasoning for its resolution.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":285,"models":[{"model_name":"BERT_base_uncased","paper_link":"https://arxiv.org/pdf/1810.04805","github_link":"https://github.com/vid-koci/bert-commonsense","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/WSC-ACL-kocijan2019surprisingly/images/sha256-c5c4d54ef4fcd46cc7da95023db48237b0a0de4dc7215d94629074b030d6cc61?context=explore","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":56}},{"model_name":"BERT_large_uncased","paper_link":"https://arxiv.org/pdf/1810.04805","github_link":"https://github.com/vid-koci/bert-commonsense","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/WSC-ACL-kocijan2019surprisingly/images/sha256-c5c4d54ef4fcd46cc7da95023db48237b0a0de4dc7215d94629074b030d6cc61?context=explore","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":61.9}},{"model_name":"BERT_wiki","paper_link":"https://arxiv.org/abs/1905.06290","github_link":"https://github.com/vid-koci/bert-commonsense","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/WSC-ACL-kocijan2019surprisingly/images/sha256-c5c4d54ef4fcd46cc7da95023db48237b0a0de4dc7215d94629074b030d6cc61?context=explore","paper_name":"A Surprisingly Robust Trick for Winograd Schema Challenge","metric":{"Accuracy":63.4}},{"model_name":"BERT_wiki_wscr","paper_link":"https://arxiv.org/abs/1905.06290","github_link":"https://github.com/vid-koci/bert-commonsense","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/WSC-ACL-kocijan2019surprisingly/images/sha256-c5c4d54ef4fcd46cc7da95023db48237b0a0de4dc7215d94629074b030d6cc61?context=explore","paper_name":"A Surprisingly Robust Trick for Winograd Schema Challenge","metric":{"Accuracy":71.8}},{"model_name":"HNN","paper_link":"https://arxiv.org/abs/1907.11983","github_link":"https://github.com/namisan/mt-dnn","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/WSC-arxiv-he2019hybrid/images/sha256-3bc75a53777ff864d0e9acd377e6c55842eaeea9ec75b182ef35194237f7005e?context=explore","paper_name":"Hybrid Neural Network Model for Commonsense Reasoning","metric":{"Accuracy":75.1}}]}]')},"0550":function(e){e.exports=JSON.parse('[{"name":"Yelp-Review","description":"Millions of document level text collected from yelp. There are five classes of dataset. The classes represent the user\'s rating of the store.","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":75.8},"dockerhub_link":""},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":73.25},"dockerhub_link":""},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":72.1},"dockerhub_link":""},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":62.05},"dockerhub_link":""},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":58.95},"dockerhub_link":""},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":66.43},"dockerhub_link":""}]},{"name":"IMDB_Large","description":"Millions of movie reviews from IMDB, Choose the most popular reviews from 20 movie categories. There are ten classes of dataset. The classes represent the user\'s rating of the movie","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":53.47},"dockerhub_link":""},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":51.53},"dockerhub_link":""},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":47.65},"dockerhub_link":""},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":47.83},"dockerhub_link":""},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":43.06},"dockerhub_link":""},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":45.33},"dockerhub_link":""}]},{"name":"Amazon-Review","description":"Five million product reviews from Amazon, There are five classes of dataset. The classes represent the user\'s rating of the product","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT-Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":75.8},"dockerhub_link":""},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":73.25},"dockerhub_link":""},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":72.1},"dockerhub_link":""},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":62.05},"dockerhub_link":""},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":58.95},"dockerhub_link":""},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":66.43},"dockerhub_link":""}]},{"name":"Reuters","description":"Reuters rcv1 and rcv2 800 000 news data, multi label classification, a total of 103 categories","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Micro-F1":90.08},"dockerhub_link":""},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Micro-F1":88.41},"dockerhub_link":""},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Micro-F1":86.85},"dockerhub_link":""},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Micro-F1":82.49},"dockerhub_link":""},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Micro-F1":78.32},"dockerhub_link":""},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Micro-F1":86.15},"dockerhub_link":""}]}]')},"08f7":function(e){e.exports=JSON.parse('{"name":"NMT","full_name":"Neural Machine Translation","description":"Neural Machine Translation is a state-of-the-art machine translation approach that utilizes neural network techniques to translate one language into another.","available_domain":["ParallelTwitterType","SwapParallelNum","SwapParallelSameWord"],"available_ut":["Typos","Ocr","Keyboard","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","WordCase-upper","WordCase-lower","WordCase-title","Punctuation"]}')},"09bc":function(e){e.exports=JSON.parse('[{"name":"LCQMC","description":"The corpus contains 260,068 question pairs with manual annotation and we split it into three parts, i.e., a training set containing 238,766 question pairs, a development set with 8,802 question pairs, and a test set with 12,500 question pairs. ","available_transformation_type":["domain","ut"],"dataset_size":260068,"models":[{"model_name":"LET","paper_link":"https://arxiv.org/pdf/2102.12671.pdf","github_link":"https://github.com/lbe0613/LET","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SMCN-AAAI-2021LET/images/sha256-61f8300f64650dcc11a14e64d0cb6f22184ac3295124241a95b677af9b32a9a1?context=explore","paper_name":"LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching","metric":{"Accuracy":87.42}},{"model_name":"Multi-Granularity Fusion Model","paper_link":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206256/pdf/978-3-030-47436-2_Chapter_19.pdf","github_link":"https://github.com/zhangxu90s/MGF","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SMCN-AAAIPress-zhang2020chinese/images/sha256-eaf928532f2bdc0f6c362abf3e1fae392146796b127124db969938a1b5cbf55d?context=explore","paper_name":"Chinese Sentence Semantic Matching Based on Multi-Granularity Fusion Model","metric":{"Accuracy":85.53}}]}]')},"0bd7":function(e){e.exports=JSON.parse('[{"name":"ChnSentiCorp","description":"The ChnSentiCorp dataset is a binary chinese sentiment analysis dataset consisting of 12000 reviews labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Models are evaluated based on Accuracy and F1 score.","available_transformation_type":["domain","ut","subpopulation"],"dataset_size":12000,"models":[{"model_name":"Chnbert","paper_link":"https://arxiv.org/abs/1906.08101","github_link":"https://github.com/ymcui/Chinese-BERT-wwm","paper_name":"Pre-Training with Whole Word Masking for Chinese BERT","metric":{"Accuracy":93.9},"dockerhub_link":"To be announced"},{"model_name":"ERNIE1.0","paper_link":"https://arxiv.org/abs/1904.09223","github_link":"https://github.com/PaddlePaddle/ERNIE","paper_name":"ERNIE: Enhanced Representation through Knowledge Integration","metric":{"Accuracy":93.9},"dockerhub_link":"To be announced"},{"model_name":"ZEN","paper_link":"https://arxiv.org/abs/1911.00720","github_link":"https://github.com/sinovation/ZEN","paper_name":"ZEN: Pre-trainingChinese Text Encoder Enhanced by N-gram Representations","metric":{"Accuracy":95.7},"dockerhub_link":"To be announced"}]}]')},"0c12":function(e){e.exports=JSON.parse('[{"name":"CoNLL 2003","description":"The CoNLL 2003 NER task consists of newswire text from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). Models are evaluated based on span-based F1 on the test set.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":3684,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":88.48},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-lample2016neural/images/sha256-22c44713298be47439ff711fc33228b060ed32ff10d56b374ef554d831091bfd?context=explore"},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":90.59},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-ma2016end/images/sha256-0c5d10a8cdbeaba2027ca4cb0f43260980cce1499ef5b8858f9e1b32a1f9d73d?context=explore"},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":90.88},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-liu2018empower/images/sha256-9fe69b6e8c833d0bd24a5afbed3d9cd30f248ad7befb930f3209bf0e94676aca?context=explore"},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":91.42},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":90.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":91.53},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-arxiv-yan2019tener/images/sha256-49fd2edf4b03c5f35f98b930575b571bcc2aeb8f8091edbe567e4ca17e9945e6?context=explore"},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":91.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-Chen2019grn/images/sha256-eaf71378fc58eda338c239ebc1b04cc37c058cf58728f69526b5ec8637df39dd?context=explore"},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":91.14},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018elmo/images/sha256-23a62a37c2e6c9fee40b3a348df5586ec1fd04680de26023c5954eb7a1f72ebe?context=explore"},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":92.25},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-COLING-Akbik2018Flair/images/sha256-3df9729fde2461327dd85355549118fce4949a5da0c32f3b1aac714c550c46ad?context=explore"},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":91.32},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018pooledflair/images/sha256-d036f7e5133d70d502d3c24093104c840b9059da73cc4aae4fbc835494741279?context=explore"},{"model_name":"BARTNER","paper_link":"https://aclanthology.org/2021.acl-long.451.pdf","github_link":"https://github.com/yhcc/BARTNER","paper_name":"A Unified Generative Framework for Various NER Subtasks","metric":{"F1":92.67},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-yan2021unified/images/sha256-aa75721da3134ce7ee97e81e8240eb5f2a0d078fea5d268421a5b72f98e28679?context=explore"},{"model_name":"UANet","paper_link":"https://aclanthology.org/2021.acl-long.451.pdf","github_link":"https://github.com/yhcc/BARTNER","paper_name":"A Unified Generative Framework for Various NER Subtasks","metric":{"F1":91.98},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-IJCAI-gui2020uncertainty/images/sha256-6c17a4e8f70cc48c3bf99c99970ae83ea22e9ddfdbfd30937781f7fc42543d87?context=explore"},{"model_name":"S-LSTM","paper_link":"https://aclanthology.org/2021.acl-long.451.pdf","github_link":"https://github.com/yhcc/BARTNER","paper_name":"A Unified Generative Framework for Various NER Subtasks","metric":{"F1":90.17},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-zhang2018sentence/images/sha256-4b5b38c1d2d69c1e5b5b88414784e42af9faa6ee7fb786ae887d9e7f3bd5dca5?context=explore"},{"model_name":"NAT","paper_link":"https://aclanthology.org/2021.acl-long.451.pdf","github_link":"https://github.com/yhcc/BARTNER","paper_name":"A Unified Generative Framework for Various NER Subtasks","metric":{"F1":92.04},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-namysl2020nat/images/sha256-508f9684ed67f74fd72f83ea8e08ed9021354b05c98c7275fc9af5eab8c707ab?context=explore"}]},{"name":"OntoNotes v5","description":"OntoNotes Release 5.0 iis comprised of 1,745k English, 900k Chinese, and 300k Arabic text data collected from a range of sources including telephone conversations, newswire, talkshows, broadcast news, broadcast conversation, and online blogs. Entities are annotated with labels such as PERSON, ORGANIZATION, and LOCATION among others.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":8469,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":82.31},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-lample2016neural/images/sha256-22c44713298be47439ff711fc33228b060ed32ff10d56b374ef554d831091bfd?context=explore"},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":85.95},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-ma2016end/images/sha256-0c5d10a8cdbeaba2027ca4cb0f43260980cce1499ef5b8858f9e1b32a1f9d73d?context=explore"},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":87.92},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-liu2018empower/images/sha256-9fe69b6e8c833d0bd24a5afbed3d9cd30f248ad7befb930f3209bf0e94676aca?context=explore"},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":88.85},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":86.85},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":88.12},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-arxiv-yan2019tener/images/sha256-49fd2edf4b03c5f35f98b930575b571bcc2aeb8f8091edbe567e4ca17e9945e6?context=explore"},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":87.84},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-Chen2019grn/images/sha256-eaf71378fc58eda338c239ebc1b04cc37c058cf58728f69526b5ec8637df39dd?context=explore"},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":85.62},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018elmo/images/sha256-23a62a37c2e6c9fee40b3a348df5586ec1fd04680de26023c5954eb7a1f72ebe?context=explore"},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":86.91},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-COLING-Akbik2018Flair/images/sha256-3df9729fde2461327dd85355549118fce4949a5da0c32f3b1aac714c550c46ad?context=explore"},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":85.57},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018pooledflair/images/sha256-d036f7e5133d70d502d3c24093104c840b9059da73cc4aae4fbc835494741279?context=explore"}]},{"name":"ACE 2005","description":"ACE 2005 Multilingual Training Corpus was developed by the Linguistic Data Consortium (LDC) and contains approximately 1,800 files of mixed genre text in English, Arabic, and Chinese annotated for entities, relations, and events.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":2050,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":82.94},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-lample2016neural/images/sha256-22c44713298be47439ff711fc33228b060ed32ff10d56b374ef554d831091bfd?context=explore"},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":82.08},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-ACL-ma2016end/images/sha256-0c5d10a8cdbeaba2027ca4cb0f43260980cce1499ef5b8858f9e1b32a1f9d73d?context=explore"},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":85.14},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-liu2018empower/images/sha256-9fe69b6e8c833d0bd24a5afbed3d9cd30f248ad7befb930f3209bf0e94676aca?context=explore"},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":87.27},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":88.75},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-devlin2018bert/images/sha256-4fe04dd44a646ace4337a7b06849c25bbdfb139abe65602e4161b703fe7f3f24?context=explore"},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":84.19},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-arxiv-yan2019tener/images/sha256-49fd2edf4b03c5f35f98b930575b571bcc2aeb8f8091edbe567e4ca17e9945e6?context=explore"},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":84.91},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-AAAI-Chen2019grn/images/sha256-eaf71378fc58eda338c239ebc1b04cc37c058cf58728f69526b5ec8637df39dd?context=explore"},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":85.42},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018elmo/images/sha256-23a62a37c2e6c9fee40b3a348df5586ec1fd04680de26023c5954eb7a1f72ebe?context=explore"},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":85.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-COLING-Akbik2018Flair/images/sha256-3df9729fde2461327dd85355549118fce4949a5da0c32f3b1aac714c550c46ad?context=explore"},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":84.65},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NER-NAACL-Akbik2018pooledflair/images/sha256-d036f7e5133d70d502d3c24093104c840b9059da73cc4aae4fbc835494741279?context=explore"}]}]')},"0f1b":function(e){e.exports=JSON.parse('[{"name":"WSJ","description":"A standard dataset for POS tagging is the Wall Street Journal (WSJ) portion of the Penn Treebank, containing 45 different POS tags. Sections 0-18 are used for training, sections 19-21 for development, and sections 22-24 for testing.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":5463,"models":[{"model_name":"CRF++","paper_link":"http://www.cis.upenn.edu/~pereira/papers/crf.pdf","github_link":"https://taku910.github.io/crfpp/","paper_name":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data","metric":{"Accuracy":95},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ICML-lafferty2001conditional/images/sha256-1cd38f277d151c4d1401d0d123a84351056ef44673145a16a0ada29fa38e49f2?context=explore"},{"model_name":"CNN-BILSTM-CRF","paper_link":"https://www.aclweb.org/old_anthology/P/P16/P16-1101.pdf","github_link":"https://github.com/XuezheMax/NeuroNLP2","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"Accuracy":97.54},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ACL-ma2016end-CNN-new/images/sha256-0b4c49162a8521727496ccc6731c85828dd55850d590760446025b5c486ae6cc?context=explore"},{"model_name":"BiLSTM-LAN","paper_link":"https://www.aclweb.org/anthology/D19-1422.pdf","github_link":"https://github.com/Nealcly/LAN","paper_name":"Hierarchically-Refined Label Attention Network for Sequence Labeling","metric":{"Accuracy":97.51},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-EMNLP-cui2019hierarchically-new/images/sha256-6a4eea683308cffd5cf30fec51b6d848a79d0e7a7974698971c9ab9029434cea?context=explore"},{"model_name":"UANet","paper_link":"https://www.aclweb.org/anthology/2020.emnlp-main.181.pdf","github_link":"https://github.com/jiacheng-ye/UANet","paper_name":"Uncertainty-Aware Label Refinement for Sequence Labeling","metric":{"Accuracy":97.66},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-EMNLP-gui2020uncertainty-new/images/sha256-28dbd4afeb3e67e4ece667793e909308cafac22aa9cc4995726ef64749c883c5?context=explore"},{"model_name":"ELMo-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N18-1202.pdf","github_link":"https://github.com/allenai/allennlp","paper_name":"Deep contextualized word representations","metric":{"Accuracy":97.75},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-NAACL-peters2018deep-new/images/sha256-c70dee70fab7543e2bbce223bbe951088963ee02603f15c4fba78d1094df92e4?context=explore"},{"model_name":"Flair-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/C18-1139/","github_link":"https://github.com/zalandoresearch/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"Accuracy":97.75},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-COLING-akbik2018contextual-new/images/sha256-2f0c5f4b617d0aceba6ca5fdc821a58763038ffc5e4744749e9f6db058d52b49?context=explore"},{"model_name":"BERT-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N19-1423.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":97.75},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ACL-devlin2018bert-new/images/sha256-2058ed20c925d2913f965a034f77aee419ae063cc1471eb9b565080a68915c98?context=explore"},{"model_name":"LM-LSTM-CRF","paper_link":"https://ojs.aaai.org/index.php/AAAI/article/view/12006/11865","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"Accuracy":97.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-AAAI-liu2018empower/images/sha256-92ab51de45b1eec6e17ff1f2d7f217c125af3e8207e7e33c085ed81e78e0ba21?context=explore"},{"model_name":"meta-tagger","paper_link":"https://aclanthology.org/P18-1246.pdf","github_link":"https://github.com/google/meta_tagger","paper_name":"Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings","metric":{"Accuracy":97.49},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ACL-bohnet2018morphosyntactic/images/sha256-00dc402e6ee4e3c8e2c933cc22ce217a0969041ab3993e7b52ff95d32beec170?context=explore"},{"model_name":"S-LSTM","paper_link":"https://arxiv.org/pdf/1805.02474.pdf","github_link":"https://github.com/leuchine/S-LSTM","paper_name":"Sentence-State LSTM for Text Representation","metric":{"Accuracy":97.27},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ACL-zhang2018sentence/images/sha256-4a5ed2b8324cc9d54f4391475ddbc9376bd0ef39684072a5b3bf1710f36aba46?context=explore"},{"model_name":"BiLSTM-aux","paper_link":"https://arxiv.org/pdf/1604.05529.pdf","github_link":"https://github.com/bplank/bilstm-aux","paper_name":"Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss","metric":{"Accuracy":97.26},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/POS-ACL-plank2016multilingual/images/sha256-26dc26b71a849603bfaba88c39da3b3a6c5423ef681caf213493bd97bada9fd3?context=explore"}]}]')},"23b9":function(e){e.exports=JSON.parse('[{"name":"PennTreebank","description":"The Penn Treebank Project annotates naturally-occuring text for linguistic structure.","available_transformation_type":["domain","ut"],"dataset_size":2405,"models":[{"model_name":"LAL","paper_link":"https://arxiv.org/abs/1911.03875","github_link":"https://github.com/KhalilMrini/LAL-Parser","paper_name":"Rethinking Self-Attention: Towards Interpretability in Neural Parsing","metric":{"LAS":96.26},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-EMNLP-mrini2019rethinking/images/sha256-b1fba3be5c8350e739da3628c0021ffed4b846d13e83e0fdaf2699bed1712b85"},{"model_name":"HPSG-BERT","paper_link":"https://arxiv.org/abs/1907.02684","github_link":"https://github.com/DoodleJZ/HPSG-Neural-Parser","paper_name":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank","metric":{"LAS":95.43},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-ACL-zhou2019head/images/sha256-69fe44b1707c5089cffdf98b2ed1d1e3052b762f73ef62b7252ff91ae97feddb"},{"model_name":"HPSG-CWT","paper_link":"https://arxiv.org/abs/1907.02684","github_link":"https://github.com/DoodleJZ/HPSG-Neural-Parser","paper_name":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank","metric":{"LAS":94.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-ACL-zhou2019head/images/sha256-69fe44b1707c5089cffdf98b2ed1d1e3052b762f73ef62b7252ff91ae97feddb"},{"model_name":"jPTDP","paper_link":"http://www.aclweb.org/anthology/K18-2008","github_link":"https://github.com/datquocnguyen/jPTDP","paper_name":"An improved neural network model for joint POS tagging and dependency parsing","metric":{"LAS":92.87},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-CONLL-nguyen2018improved/images/sha256-10a9f45964950754893692caeec2fe3a5fd5e2d5264894e8d61f3dbd2fbb7284"},{"model_name":"crf2o","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.302","github_link":"https://github.com/yzhangcs/parser","paper_name":"Efficient Second-Order TreeCRF for Neural Dependency Parsing","metric":{"LAS":94.51},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-ACL-zhang2020efficient/images/sha256-6b35653d86c1d3d8c13aba6e2c031246a7b8ef9050bc1d5954ae7243bdc4bbf9"},{"model_name":"biaffine","paper_link":"https://arxiv.org/abs/1611.01734","github_link":"https://github.com/yzhangcs/parser","paper_name":"Deep Biaffine Attention for Neural Dependency Parsing","metric":{"LAS":94.41},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-ICLR-dozat2016deep/images/sha256-4647204ae26f8eea1f99603421f502271fcd0a6a823fa20746c43eb4501652a9"},{"model_name":"biaffine-roberta","paper_link":"https://arxiv.org/abs/1611.01734","github_link":"https://github.com/yzhangcs/parser","paper_name":"Deep Biaffine Attention for Neural Dependency Parsing","metric":{"LAS":95.86},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/DP-ICLR-dozat2016deep/images/sha256-4647204ae26f8eea1f99603421f502271fcd0a6a823fa20746c43eb4501652a9"}]}]')},"314d":function(e){e.exports=JSON.parse('[{"name":"Tacred","description":"Tacred is a relation extraction dataset with around 15,000 sentences","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":15509,"models":[{"model_name":"LSTM-ATT","paper_link":"https://www.aclweb.org/anthology/D17-1004.pdf","github_link":"https://github.com/yuhaozhang/tacred-relation","paper_name":"Position-aware Attention and Supervised Data Improve Slot Filling","metric":{"F1":65.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-EMNLP-zhang2017position/images/sha256-0a5891a6af5e7fee13fd067be9fd5fd13dddabbfbba329e10e210173e21d74d6?context=explore"},{"model_name":"GCN","paper_link":"https://www.aclweb.org/anthology/D18-1244.pdf","github_link":"https://github.com/qipeng/gcn-over-pruned-trees","paper_name":"Graph Convolution over Pruned Dependency Trees Improves Relation Extraction","metric":{"F1":62.03},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-EMNLP-zhang2018graph/images/sha256-e71ba8c49bf29ebed9b96c74ae5cba45af6549e8fe0f8709bf70a3ca4dd00b38?context=explore"},{"model_name":"AGGCN","paper_link":"https://www.aclweb.org/anthology/P19-1024.pdf","github_link":"https://github.com/Cartus/AGGCN","paper_name":"Attention Guided Graph Convolutional Networks for Relation Extraction","metric":{"F1":67.41},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-ACL-guo2019aggcn/images/sha256-cd03dca8ec8f450a66fc2cba58224a1611c21c24c9e6822348e106ba64960ed6?context=explore"},{"model_name":"BERT-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":68.01},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-NAACL-devlin2019bert/images/sha256-c4d45371fbe70e42447556cd33c667f710bde3f12a52676110ec5b138d19ccf6?context=explore"},{"model_name":"BERT-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":67.73},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-NAACL-devlin2019bert/images/sha256-c4d45371fbe70e42447556cd33c667f710bde3f12a52676110ec5b138d19ccf6?context=explore"},{"model_name":"CP-finetune","paper_link":"https://aclanthology.org/2020.emnlp-main.298.pdf","github_link":"https://github.com/thunlp/RE-Context-or-Names","paper_name":"Learning from Context or Names? An Empirical Study on Neural Relation Extraction","metric":{"F1":68.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-EMNLP-peng2020learning/images/sha256-756006148ce66c1f3959384284374a4d047a2e64b035ee8c72d184fcb7ea116a?context=explore"},{"model_name":"KD4NRE","paper_link":"https://ojs.aaai.org/index.php/AAAI/article/view/6509","github_link":"https://github.com/zzysay/KD4NRE","paper_name":"Distilling Knowledge from Well-informed Soft Labels for Neural Relation Extraction","metric":{"F1":68.16},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-AAAI-zhang2020distilling/images/sha256-b92248b57cc1c4127f5996bf0655c71db576367993a4870db4514bcd405b098a?context=explore"},{"model_name":"LUKE","paper_link":"https://aclanthology.org/2020.emnlp-main.523.pdf","github_link":"https://github.com/studio-ousia/luke","paper_name":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","metric":{"F1":72.66},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-EMNLP-yamada2020luke/images/sha256-fbbe391256f450b7e65645d9af42ec780ff5ff5485eeef2a8ed6826226596c85?context=explore"},{"model_name":"SPANBERT","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"F1":70.81},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-TACL-joshi2020spanbert/images/sha256-3567ebdfc08c0b792d0ea11b2c2b861a005ca572dea760b20fb1507b7b58dde0?context=explore"},{"model_name":"TRE","paper_link":"https://arxiv.org/abs/1906.03088","github_link":"https://github.com/DFKI-NLP/TRE","paper_name":"Improving Relation Extraction by Pre-trained Language Representations","metric":{"F1":67.42},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-AKBC-alt2019improving/images/sha256-ccc9b0425a3e076f8250924467993b99a8754c09db7058985d67d02e2ddb02a0?context=explore"}]},{"name":"NYT","description":"NYT is a distantly supervised relation extraction dataset with around 172,000 sentences","dataset_size":172448,"available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"models":[{"model_name":"PCNN-ATT","paper_link":"https://www.aclweb.org/anthology/P16-1200.pdf","github_link":"https://github.com/thunlp/OpenNRE","paper_name":"Neural Relation Extraction with Selective Attention over Instances","metric":{"AUC":0.3366,"F1":0.412},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-ACL-lin2016neural/images/sha256-a063a66d34c05ac49fee1d9eccccb91c44c8214b9d63375d19d4b49ed296731f?context=explore"},{"model_name":"PCNN+ATT RA+BAG ATT","paper_link":"https://arxiv.org/abs/1904.00143","github_link":"https://github.com/ZhixiuYe/Intra-Bag-and-Inter-Bag-Attentions","paper_name":"Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions","metric":{"AUC":0.4212,"F1":0.4555},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-NAACL-ye-ling-2019-distant/images/sha256-19f4fd7935896ba5dbf5270c69fe02bdc8effb9361e1003a5e672b9011be8ec5?context=explore"},{"model_name":"PCNN","paper_link":"https://www.aclweb.org/anthology/D15-1203.pdf","github_link":"https://github.com/thunlp/OpenNRE","paper_name":"Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks","metric":{"AUC":0.3463,"F1":0.4087},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-EMNLP-zeng2015distant/images/sha256-bdcc438bec27362790cad5991640021943b0b3ef9f3743d2c2ecf3a36ebc45eb?context=explore"},{"model_name":"SeG","paper_link":"https://arxiv.org/pdf/1911.11899.pdf","github_link":"https://github.com/tmliang/SeG","paper_name":"Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction","metric":{"AUC":0.4575,"F1":0.4927},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-AAAI-li2020self/images/sha256-a6ad3acd7a87598be9df03f0f7606c75f5544aa545182b8380b650d3514a4623?context=explore"},{"model_name":"dsre-vae","paper_link":"https://aclanthology.org/2021.naacl-main.2.pdf","github_link":"https://github.com/fenchri/dsre-vae","paper_name":"Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors","metric":{"AUC":0.4024,"F1":0.4449},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-arxiv-liang2021distantly/images/sha256-a15219fd4c72d29cbf6459952a27f4651a341114e7a285780e12f85d658a5f16?context=explore"},{"model_name":"DISTRE","paper_link":"https://www.aclweb.org/anthology/P19-1134","github_link":"https://github.com/DFKI-NLP/DISTRE","paper_name":"Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","metric":{"AUC":0.4225,"F1":0.4864},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/RE-ACL-alt-etal-2019-fine/images/sha256-f55d99f4c52eddcae763eb2e67cb12b5c53cbf590d6ade032c5aebbb9473689c?context=explore"}]}]')},"4a32":function(e){e.exports=JSON.parse('{"name":"WSD","full_name":"Word Sense Disambiguation","description":"Word Sense Disambiguation(WSD) is a task that aims to find the most suitable sense for each target word in a given context.","available_domain":["SwapTarget-syn"],"available_ut":["Contraction","AppendIrr","InsertAdv","Keyboard","MLMSuggestion","Ocr","Punctuation","ReverseNeg","SpellingError","SwapAntWordNet","SwapNamedEnt","SwapNum","SwapSynWordEmbedding","SwapSynWordNet","Tense","TwitterType","Typos","WordCase-upper"]}')},"4e26":function(e){e.exports=JSON.parse('[{"name":"SensEval2","description":"This dataset consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives.","available_transformation_type":["domain","ut"],"dataset_size":242,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":79.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":79.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":81},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":77.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":76.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":79.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":78},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":79.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":79.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":82.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]},{"name":"SensEval3","description":"This datasets is divided in three documents from three different domains (editorial, news story and fiction), totaling 1850 sense annotations.","available_transformation_type":["domain","ut"],"dataset_size":252,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":77.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":77.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":79},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":75.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":75.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":77.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":77.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":79.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":78.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":79.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]},{"name":"SemEval2007","description":"This is the smallest among the five datasets, containing 455 sense annotations for nouns and verbs only.","available_transformation_type":["domain","ut"],"dataset_size":135,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":74.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":72.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":75.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":72.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":68.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":73.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":68.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":71.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":74.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":77.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]},{"name":"SemEval2013","description":"This dataset includes thirteen documents from various domains. In this case the original sense inventory was WordNet 3.0, which is the same that we use for all datasets. The number of sense annotations is 1644, although only nouns are considered.","available_transformation_type":["domain","ut"],"dataset_size":306,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":79.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":79.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":80.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":76.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":75.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":78.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":76.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":78.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":80},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":83.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]},{"name":"SemEval2015","description":"This is the most recent WSD dataset available to date. It consists of 1022 sense annotations in four documents coming from three heterogeneous domains: biomedical, mathematics/computing and social issues.","available_transformation_type":["domain","ut"],"dataset_size":138,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":81.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":84.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":81.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":79.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":77},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":82.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":76.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":83.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":82},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":85.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]},{"name":"ALL","description":"This dataset is the concatenations of all SensEval2,SensEval3,SemEval2007,SemEval2013 and SemEval2015 as a single evaluation dataset","available_transformation_type":["domain","ut"],"dataset_size":1173,"models":[{"model_name":"BEM","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.95/","github_link":"https://github.com/facebookresearch/wsd-biencoders","paper_name":"Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders","metric":{"F1":79},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-blevins2020moving/images/sha256-f0cfc68b7bea0bc7dfc8e6766a5962874773da25836806a7dd4aaac3490c2b2a?context=explore"},{"model_name":"BERT-large-augmented","paper_link":"https://arxiv.org/abs/2009.11795","github_link":"https://github.com/BPYap/BERT-WSD","paper_name":"Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences","metric":{"F1":79.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-yap2020adapting/images/sha256-51de58b4241ef9b79b393743a5b3e945c70e5cdca6b8d7fd6623c9cc9941fcd5?context=explore"},{"model_name":"EWISER","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.255/","github_link":"https://github.com/SapienzaNLP/ewiser","paper_name":"Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information","metric":{"F1":80.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-bevilacqua2020breaking/images/sha256-9186288b5806a60e44f5af703903244f19f11ff03fde0df762e68ebb27b9a197?context=explore"},{"model_name":"GlossBERT(Sent-CLS-WS)","paper_link":"https://arxiv.org/abs/1908.07245","github_link":"https://github.com/HSLCY/GlossBERT","paper_name":"GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge","metric":{"F1":76.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-huang2019glossbert/images/sha256-bae2f091bafb81013cffe8381245fe93e0ca8aaf3fc1188f75e8807996d62c99?context=explore"},{"model_name":"LMMS2348","paper_link":"https://www.aclweb.org/anthology/P19-1569/","github_link":"https://github.com/danlou/LMMS","paper_name":"Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation","metric":{"F1":75.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-loureiro2019language/images/sha256-bfd9371cd722bcebf919af12b4259632e280773f0c746e6b68dcb84f27e23c78?context=explore"},{"model_name":"SemCor+WNGC,hypernyms","paper_link":"https://arxiv.org/abs/1905.05677","github_link":"https://github.com/getalp/disambiguate","paper_name":"https://arxiv.org/abs/1905.05677","metric":{"F1":79},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-arXiv-vial2019sense/images/sha256-bbdbc7ab45074114b498361241ccb180a887e28539156fbfa85c7a99f0b36a52?context=explore"},{"model_name":"SparseLMMS+WNGC","paper_link":"https://aclanthology.org/2020.emnlp-main.683/","github_link":"https://github.com/begab/sparsity_makes_sense","paper_name":"Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations","metric":{"F1":76.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-berend2020sparsity/images/sha256-5e946fb2f42fec97e4cf1fd56a8e445166fc42def4a5c86fc7d004098dcd4e48?context=explore"},{"model_name":"SACE","paper_link":"https://aclanthology.org/2021.acl-long.406/","github_link":"https://github.com/lwmlyy/SACE","paper_name":"[ACL 2021] Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives","metric":{"F1":79.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-ACL-wang2021word/images/sha256-0550a1e1afcfaac1f6088a89edd3d10214c8afd4ce1c6dcefd4493b630ded0d5?context=explore"},{"model_name":"ESR","paper_link":"https://aclanthology.org/2021.findings-emnlp.365/","github_link":"https://github.com/nusnlp/esr","paper_name":"[EMNLP 2021] Improved Word Sense Disambiguation with Enhanced Sense Representations","metric":{"F1":80},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-song2021improved/images/sha256-89a36c7fc8cdfe01993f1051cfe797db1bae436440b11812a9ab8b2203194af9?context=explore"},{"model_name":"ConSeC","paper_link":"https://aclanthology.org/2021.emnlp-main.112/","github_link":"https://github.com/SapienzaNLP/consec","paper_name":"[EMNLP 2021] ConSeC: Word Sense Disambiguation as Continuous Sense Comprehension","metric":{"F1":81.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/WSD-EMNLP-barba2021consec/images/sha256-4e4fbd72b4fa5b4ed4dd0bf0a05a128998f74d758695330b7f25a09a1b9203fb?context=explore"}]}]')},"4ed0":function(e){e.exports=JSON.parse('{"name":"NER","full_name":"Named Entity Recognition","description":"Named entity recognition is the task of tagging entities in text with their corresponding type.","available_domain":["ConcatSent","SwapEnt-CrossCategory","EntTypos","SwapEnt-OOV","SwapEnt-SwapLonger"],"available_ut":["RevNeg","AddRmvPunc","AppendIrr","Contraction","InsertAdv","Keyboard","MLMSuggestion","Ocr","SpellingError","SwapAnt-WordNet","SwapNum","SwapSyn-WordEmbedding","SwapSyn-WordNet","Tense","TwitterType","Typos-random","WordCase-lower","WordCase-title","WordCase-upper","WordCase"]}')},6683:function(e){e.exports=JSON.parse('[{"name":"Ontonotes","description":"OntoNotes Release 5.0","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"models":[{"model_name":"c2f+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-2108","github_link":"https://github.com/kentonl/e2e-coref","paper_name":"Higher-order Coreference Resolution with Coarse-to-fine Inference","metric":{"Accuracy":72.39},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-NAACL-lee2018higher/images/sha256-e46a49d6f49e3173efd20ea5c31dff7a8c5e9da078f9f59669ed5e03dd7b6eed?context=explore"},{"model_name":"e2e+ELMo","paper_link":"https://www.aclweb.org/anthology/D17-1018/","github_link":"https://github.com/kentonl/e2e-coref/releases/tag/e2e","paper_name":"End-to-end Neural Coreference Resolution","metric":{"Accuracy":67.23},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-EMNLP-lee2017end/images/sha256-0960d8f2d0a54d88fde6e55f43fcbb5c56adf45e3d74a75707d0b8ff35521a31?context=explore"},{"model_name":"c2f+BERT","paper_link":"https://www.aclweb.org/anthology/D19-1588","github_link":"https://github.com/mandarjoshi90/coref","paper_name":"BERT for Coreference Resolution: Baselines and Analysis","metric":{"Accuracy":74.74},"dockerhub_link":""},{"model_name":"c2f+SpanBERT","paper_link":"https://www.aclweb.org/anthology/2020.tacl-1.5","github_link":"https://github.com/mandarjoshi90/coref","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":77.41},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-TACL-joshi2020spanbert/images/sha256-3567ebdfc08c0b792d0ea11b2c2b861a005ca572dea760b20fb1507b7b58dde0?context=explore"},{"model_name":"s2e+Longformer-Large","paper_link":"https://arxiv.org/abs/2101.00434","github_link":"https://github.com/yuvalkirstain/s2e-coref","paper_name":"[ACL 2021] Coreference Resolution without Span Representations","metric":{"Accuracy":80.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-ACL-kirstain2021coreference/images/sha256-98d8a89d59aab32928516b0cf37db5bb3ce1e72eae86805385c5ef8b93d80078?context=explore"},{"model_name":"ICOREF","paper_link":"https://arxiv.org/abs/2104.08457","github_link":"https://github.com/pitrack/incremental-coref/","paper_name":"[EMNLP 2021] Moving on from OntoNotes: Coreference Resolution Model Transfer","metric":{"Accuracy":78.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/COREF-EMNLP-xia2021moving/images/sha256-102f3c3634e85a1be3d1a42ab1bd65a2d638f75089d59143d7304c140c9f860c?context=explore"}]}]')},7262:function(e){e.exports=JSON.parse('[{"name":"SemEval2014-Restaurant","description":"The standard SemEval2014-Restaurant dataset consists of 3,452 training, 150 validation, and 1,120 test English sentences from the restaurant reviews. Our task-specfic transformations are based on SemEval2014-Restaurant-TOWE, which provides opinion words and their position. The test set of SemEval2014-Restaurant-TOWE owns 492 different sentences (847 aspect terms).","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":1120,"models":[{"model_name":"LCF-BERT","paper_link":"https://www.researchgate.net/publication/335238076_LCF_A_Local_Context_Focus_Mechanism_for_Aspect-Based_Sentiment_Classification","github_link":"https://github.com/yangheng95/LC-ABSA","paper_name":"LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification","metric":{"Accuracy":84.82,"Macro-F1":76.99},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-AppliedScience-zeng2019lcf/images/sha256-e8437267241ba4570a31a3af2974f83faaf66fb8e8ba76ef59b2bed2aed75d2d?context=explore"},{"model_name":"BERT+ASPECT","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/bert_spc.py","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":85.18,"Macro-F1":77.43},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NAACL-devlin2018bert-BERTSPC/images/sha256-e5709475dca7a50fa1a0be533a3649a71219bae98d43a3fbdc4945ba04ccc96b?context=explore"},{"model_name":"BERT-BASE","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":79.02,"Macro-F1":69.17},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NAACL-devlin2018bert-BERTBASE/images/sha256-8ed800d0b3f0aebbe03f21cddbf9c4582d96f132103a9b93f33e793fb1ce4602?context=explore"},{"model_name":"MGAN","paper_link":"https://www.aclweb.org/anthology/D18-1380/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/mgan.py","paper_name":"Multi-grained Attention Network for Aspect-Level Sentiment Classification","metric":{"Accuracy":79.11,"Macro-F1":67.23},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-fan2018multi/images/sha256-ef89823f2a3cba59fca3389e3ea293ab91b149031c6cf1ea8450a1a65352275c?context=explore"},{"model_name":"TNet","paper_link":"https://arxiv.org/pdf/1805.01086","github_link":"https://github.com/lixin4ever/TNet","paper_name":"Transformation Networks for Target-Oriented Sentiment Classification","metric":{"Accuracy":78.93,"Macro-F1":67.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-ACL-li2018transformation/images/sha256-03c53b0123353605ded81bd987044beebea178a2aadbee668c13536b378f895c?context=explore"},{"model_name":"IAN","paper_link":"https://arxiv.org/pdf/1709.00893","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/ian.py","paper_name":"Interactive Attention Networks for Aspect-Level Sentiment Classification","metric":{"Accuracy":76.43,"Macro-F1":63.78},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-IJCAI-ma2017interactive/images/sha256-991ccdc6a47f5cf6ee4f422289c7942a17fa2cbcf6dd0b862406fca8a7e509c4?context=explore"},{"model_name":"MemNet","paper_link":"https://arxiv.org/pdf/1605.08900","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/memnet.py","paper_name":"Aspect Level Sentiment Classification with Deep Memory Network","metric":{"Accuracy":75.71,"Macro-F1":62.23},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-tang2016aspect/images/sha256-b539e4cd594b24c2bdd1e7170de41d36a6aecce36736a7917ad3d291380ef44d?context=explore"},{"model_name":"ATAE-LSTM","paper_link":"https://www.aclweb.org/anthology/D16-1058/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/atae_lstm.py","paper_name":"Attention-based lstm for aspect-level sentiment classification","metric":{"Accuracy":77.41,"Macro-F1":63.28},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-wang2016attention/images/sha256-6c58af6a535164cdd4eedb6231339cea4960dd48345a6e37dda4d7f34aec35f9?context=explore"},{"model_name":"TD-LSTM","paper_link":"https://arxiv.org/pdf/1512.01100","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/td_lstm.py","paper_name":"Effective LSTMs for Target-Dependent Sentiment Classification","metric":{"Accuracy":77.59,"Macro-F1":63.28},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-ACL-tang2015effective/images/sha256-25c67ecac6eab4e69897c75962786e44d3cb95729779b50e2826ae3b13909965?context=explore"},{"model_name":"LSTM","paper_link":"https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lstm.py","paper_name":"Long short-term memory","metric":{"Accuracy":76.34,"Macro-F1":63.66},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NeuralComputation-hochreiter1997long/images/sha256-9a18b514e907fecaaf5d7ae7b049f908234ec496f39c1612a58f02af5292f62f?context=explore"},{"model_name":"AEN-BERT","paper_link":"https://arxiv.org/pdf/1902.09314.pdf","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/aen.py","paper_name":"Attentional Encoder Network for Targeted Sentiment Classification","metric":{"Accuracy":81.34,"Macro-F1":69.59},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-arXiv-song2019attentional-AENBERT/images/sha256-8ea251802086f8ecd9630d9bc74c8bf7d754cac70a430ae616bedb0e57455f2d?context=explore"},{"model_name":"AEN-Glove","paper_link":"https://arxiv.org/pdf/1902.09314.pdf","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/aen.py","paper_name":"Attentional Encoder Network for Targeted Sentiment Classification","metric":{"Accuracy":75.18,"Macro-F1":57.39},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-arXiv-song2019attentional-AENGloVe/images/sha256-d6a6da32d8380af059a0976fb47219f8444933463da07becca98d1b6b05bb62f?context=explore"}]},{"name":"SemEval2014-Laptop","description":"The SemEval2014-Laptop dataset consists of 2,163 training, 150 validation, and 638 test English sentences extracted from customer reviews of laptops. Our task-specfic transformations are based on SemEval2014-Laptop-TOWE, which provides opinion words and their position. The test set of SemEval2014-Laptop-TOWE owns 331 different sentences (446 aspect terms).","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":638,"models":[{"model_name":"LCF-BERT","paper_link":"https://www.researchgate.net/publication/335238076_LCF_A_Local_Context_Focus_Mechanism_for_Aspect-Based_Sentiment_Classification","github_link":"https://github.com/yangheng95/LC-ABSA","paper_name":"LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification","metric":{"Accuracy":80.72,"Macro-F1":77.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-AppliedScience-zeng2019lcf/images/sha256-e8437267241ba4570a31a3af2974f83faaf66fb8e8ba76ef59b2bed2aed75d2d?context=explore"},{"model_name":"BERT+ASPECT","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/bert_spc.py","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":76.02,"Macro-F1":71.03},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NAACL-devlin2018bert-BERTSPC/images/sha256-e5709475dca7a50fa1a0be533a3649a71219bae98d43a3fbdc4945ba04ccc96b?context=explore"},{"model_name":"BERT-BASE","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":78.21,"Macro-F1":73.16},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NAACL-devlin2018bert-BERTBASE/images/sha256-8ed800d0b3f0aebbe03f21cddbf9c4582d96f132103a9b93f33e793fb1ce4602?context=explore"},{"model_name":"MGAN","paper_link":"https://www.aclweb.org/anthology/D18-1380/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/mgan.py","paper_name":"Multi-grained Attention Network for Aspect-Level Sentiment Classification","metric":{"Accuracy":72.41,"Macro-F1":65.62},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-fan2018multi/images/sha256-ef89823f2a3cba59fca3389e3ea293ab91b149031c6cf1ea8450a1a65352275c?context=explore"},{"model_name":"TNet","paper_link":"https://arxiv.org/pdf/1805.01086","github_link":"https://github.com/lixin4ever/TNet","paper_name":"Transformation Networks for Target-Oriented Sentiment Classification","metric":{"Accuracy":71.16,"Macro-F1":65.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-ACL-li2018transformation/images/sha256-03c53b0123353605ded81bd987044beebea178a2aadbee668c13536b378f895c?context=explore"},{"model_name":"IAN","paper_link":"https://arxiv.org/pdf/1709.00893","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/ian.py","paper_name":"Interactive Attention Networks for Aspect-Level Sentiment Classification","metric":{"Accuracy":70.53,"Macro-F1":63.48},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-IJCAI-ma2017interactive/images/sha256-991ccdc6a47f5cf6ee4f422289c7942a17fa2cbcf6dd0b862406fca8a7e509c4?context=explore"},{"model_name":"MemNet","paper_link":"https://arxiv.org/pdf/1605.08900","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/memnet.py","paper_name":"Aspect Level Sentiment Classification with Deep Memory Network","metric":{"Accuracy":64.11,"Macro-F1":55.87},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-tang2016aspect/images/sha256-b539e4cd594b24c2bdd1e7170de41d36a6aecce36736a7917ad3d291380ef44d?context=explore"},{"model_name":"ATAE-LSTM","paper_link":"https://www.aclweb.org/anthology/D16-1058/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/atae_lstm.py","paper_name":"Attention-based lstm for aspect-level sentiment classification","metric":{"Accuracy":69.91,"Macro-F1":64.08},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-EMNLP-wang2016attention/images/sha256-6c58af6a535164cdd4eedb6231339cea4960dd48345a6e37dda4d7f34aec35f9?context=explore"},{"model_name":"TD-LSTM","paper_link":"https://arxiv.org/pdf/1512.01100","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/td_lstm.py","paper_name":"Effective LSTMs for Target-Dependent Sentiment Classification","metric":{"Accuracy":68.34,"Macro-F1":63.22},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-ACL-tang2015effective/images/sha256-25c67ecac6eab4e69897c75962786e44d3cb95729779b50e2826ae3b13909965?context=explore"},{"model_name":"LSTM","paper_link":"https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lstm.py","paper_name":"Long short-term memory","metric":{"Accuracy":70.38,"Macro-F1":62.59},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-NeuralComputation-hochreiter1997long/images/sha256-9a18b514e907fecaaf5d7ae7b049f908234ec496f39c1612a58f02af5292f62f?context=explore"},{"model_name":"AEN-BERT","paper_link":"https://arxiv.org/pdf/1902.09314.pdf","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/aen.py","paper_name":"Attentional Encoder Network for Targeted Sentiment Classification","metric":{"Accuracy":78.84,"Macro-F1":74.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-arXiv-song2019attentional-AENBERT/images/sha256-8ea251802086f8ecd9630d9bc74c8bf7d754cac70a430ae616bedb0e57455f2d?context=explore"},{"model_name":"AEN-Glove","paper_link":"https://arxiv.org/pdf/1902.09314.pdf","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/aen.py","paper_name":"Attentional Encoder Network for Targeted Sentiment Classification","metric":{"Accuracy":66.14,"Macro-F1":57.39},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/ABSA-arXiv-song2019attentional-AENGloVe/images/sha256-d6a6da32d8380af059a0976fb47219f8444933463da07becca98d1b6b05bb62f?context=explore"}]}]')},"7aec":function(e){e.exports=JSON.parse('{"name":"SM","full_name":"Semantic Matching","description":"This task need model to identify whether two sentences express the same meaning.","available_domain":["SwapWord","SwapNum","Overlap"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RmvPunc","AddNeg","RmvNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType","BackTrans","Prejudge"]}')},"7b5f":function(e){e.exports=JSON.parse('{"name":"POS","full_name":"Part-of-speech tagging","description":"Part-of-speech tagging (POS tagging) is the task of tagging a word in a text with its part of speech. A part of speech is a category of words with similar grammatical properties. Common English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.","available_domain":["SwapMultiPOS-NN","SwapMultiPOS-JJ","SwapMultiPOS-VB","SwapMultiPOS-RB","SwapPrefix"],"available_ut":["InsertAdv","AppendIrr","WordCase-lower","WordCase-title","WordCase-upper","Contraction","SwapNamedEnt","Keyboard","MLMSuggestion","SwapNum","Ocr","AddPunc","RevNeg","SpellingError","TwitterType","Typos","SwapSyn-WordEmbedding","SwapAnt-WordNet","SwapSyn-WordNet"]}')},"7c1f":function(e){e.exports=JSON.parse('[{"name":"CTB6","description":"CTB6(Chinese Treebank 6.0) is the latest version produced from the Chinese Treebank project, consisting of 780,000 words (over 1.28 million Chinese characters) that are segmented, part-of-speech tagged and fully bracketed. The data sources include newswire from Xinhua News Agency, articles from Sinorama Magazine, news from the website of the Hong Kong Special Administrative Region and transcripts from various broadcast news programs.","available_transformation_type":["domain","domain_domain"],"dataset_size":2079,"models":[{"model_name":"MCCWS","paper_link":"https://arxiv.org/abs/1906.12035","github_link":"https://github.com/acphile/MCCWS","paper_name":"A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder","metric":{"Macro-F1":93.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-EMNLP-qiu2019concise/images/sha256-47601df599141df2b6c502ac775af17305a4f80890a85ed72deb749875f1475a?context=explore"},{"model_name":"Sub-CWS","paper_link":"https://arxiv.org/abs/1810.12594","github_link":"https://github.com/jiesutd/SubwordEncoding-CWS","paper_name":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation","metric":{"Macro-F1":96.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-NAACL-yang2018subword/images/sha256-c91b8328d07e86f5f1ae0e51e6e746b688d023e9fedece0be683951356a94062?context=explore"},{"model_name":"GreedyCWS","paper_link":"https://arxiv.org/abs/1704.07047","github_link":"https://github.com/jcyk/greedyCWS","paper_name":"Fast and Accurate Neural Word Segmentation for Chinese","metric":{"Macro-F1":95.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-ACL-cai2017fast/images/sha256-05d567fbc3b47216bdf4867700f89e649264d92bb4287911a59fa8a064a8f51f?context=explore"},{"model_name":"CWS-LSTM","paper_link":"Long Short-Term Memory Neural Networks for Chinese Word Segmentation","github_link":"https://github.com/FudanNLP/CWS_LSTM","paper_name":"https://www.aclweb.org/anthology/D15-1141/","metric":{"Macro-F1":95.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-EMNLP-chen2015long/images/sha256-1eda145d04e4778879903ff8af252f1d8dfb8ba04a51bda98c02446e3b40071a?context=explore"},{"model_name":"CWS","paper_link":"https://arxiv.org/abs/1606.04300","github_link":"https://github.com/jcyk/CWS","paper_name":"Neural Word Segmentation Learning for Chinese","metric":{"Macro-F1":95},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-ACL-cai2016neural/images/sha256-020f59b13e25333808237785f9a9823eafaee161d7c40723a30f372541d3360a?context=explore"},{"model_name":"CRF","paper_link":"","github_link":"https://github.com/wellecks/cws","paper_name":"","metric":{"Macro-F1":94.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-CRF/images/sha256-61dd24cd1ba2e75c0081faea3f7117a65aa0b5419c4e9d960f0ef97b86a27c26?context=explore"},{"model_name":"FMM","paper_link":"","github_link":"https://github.com/minixalpha/PyCWS","paper_name":"","metric":{"Macro-F1":86.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-FMM/images/sha256-a9a9be07df40c46e6643a91dd6da70f2f3912617ef0a5072fe17b148fdf120c0?context=explore"},{"model_name":"BMM","paper_link":"","github_link":"https://github.com/minixalpha/PyCWS","paper_name":"","metric":{"Macro-F1":85.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-BMM/images/sha256-85f1df66ed6a0d5559edb5f509de32eeba4e76cbe72f86e67f588ef136a0d58a?context=explore"},{"model_name":"DictConcatModel","paper_link":"https://ojs.aaai.org/index.php/AAAI/article/view/11959","github_link":"https://github.com/CPF-NLPR/CWS_Dict","paper_name":"Neural Networks Incorporating Dictionaries for Chinese Word Segmentation","metric":{"Macro-F1":96.1},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-AAAI-zhang2018neural/images/sha256-6cb54585d74e34e1019c298c7065296d80fd1ac45be45a957b0a4a40e2dc5dfa?context=explore"},{"model_name":"DictHyperModel","paper_link":"https://ojs.aaai.org/index.php/AAAI/article/view/11959","github_link":"https://github.com/CPF-NLPR/CWS_Dict","paper_name":"Neural Networks Incorporating Dictionaries for Chinese Word Segmentation","metric":{"Macro-F1":96.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-AAAI-zhang2018neural/images/sha256-6cb54585d74e34e1019c298c7065296d80fd1ac45be45a957b0a4a40e2dc5dfa?context=explore"},{"model_name":"McASP_Bert","paper_link":"https://aclanthology.org/2020.coling-main.187/","github_link":"https://github.com/cuhksz-nlp/McASP","paper_name":"Joint Chinese Word Segmentation and Part-of-speech Tagging via Multi-channel Attention of Character N-grams","metric":{"Macro-F1":97.56},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-ACL-tian2020joint/images/sha256-25115dd4fb5d5aaecd253bb064e35b08bd0b7341d883fe37c90e14e230557c7e?context=explore"},{"model_name":"McASP_Zen","paper_link":"https://aclanthology.org/2020.coling-main.187/","github_link":"https://github.com/cuhksz-nlp/McASP","paper_name":"Joint Chinese Word Segmentation and Part-of-speech Tagging via Multi-channel Attention of Character N-grams","metric":{"Macro-F1":97.67},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-ACL-tian2020joint/images/sha256-25115dd4fb5d5aaecd253bb064e35b08bd0b7341d883fe37c90e14e230557c7e?context=explore"},{"model_name":"WMSeg","paper_link":"https://aclanthology.org/2020.acl-main.734/","github_link":"https://github.com/SVAIGBA/WMSeg","paper_name":"Improving Chinese Word Segmentation with Wordhood Memory Networks","metric":{"Macro-F1":97.25},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CWS-COLING-tian2020joint/images/sha256-d6e916503a0d10cb8ec3eb2e3fadc5fc8fcc4ff0541eab261204a9608fae6767?context=explore"}]}]')},"871d":function(e){e.exports=JSON.parse('[{"name":"CTB5","description":"CTB5(Chinese Treebank 5.0) was developed by the Linguistic Data Consortium (LDC) contains approximately 500,000 words of Chinese newswire text annotated in the manner of the Penn English Treebank.","available_transformation_type":["ut"],"dataset_size":890,"models":[{"model_name":"DeepBiaf","paper_link":"https://arxiv.org/pdf/1611.01734.pdf","github_link":"https://github.com/XuezheMax/NeuroNLP2","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CNDP-ICLR-dozat2016deep/images/sha256-c478f2d362a7a0d744f6a4e99ac42f0352e16c96532586f8c4b9e78a04f725aa?context=explore","paper_name":"Deep Biaffine Attention for Neural Dependency Parsing","metric":{"UAS":91.78,"LAS":90.48}},{"model_name":"Neuromst","paper_link":"https://arxiv.org/pdf/1701.00874.pdf","github_link":"https://github.com/XuezheMax/NeuroNLP2","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CNDP-IJCNLP-ma2017neural/images/sha256-89e58d62dcfccb28a3ef20554fb0647a94c324014160d509944dd5cfe72e25a4?context=explore","paper_name":"Neural Probabilistic Model for Non-projective MST Parsing","metric":{"UAS":91.61,"LAS":90.38}},{"model_name":"StackPtr","paper_link":"https://arxiv.org/pdf/1805.01087.pdf","github_link":"https://github.com/XuezheMax/NeuroNLP2","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CNDP-ACL-ma2018stack/images/sha256-5e9989fdaadda8f9f0bf4e7d8d479022cc2344c584a2bf5b1a759199c92398f1?context=explore","paper_name":"Stack-Pointer Networks for Dependency Parsing","metric":{"UAS":92,"LAS":90.92}},{"model_name":"MRCforDP","paper_link":"https://arxiv.org/pdf/2105.07654.pdf","github_link":"https://github.com/ShannonAI/mrc-for-dependency-parsing","dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/CNDP-arXiv-gan2021dependency/images/sha256-5dd0b5aa25edd7e14c31a101d1bc5c3e90c1cef30d185127b304662fc1d43e1c?context=explore","paper_name":"Dependency Parsing as MRC-based Span-Span Prediction","metric":{"UAS":93.17,"LAS":91.67}}]}]')},"8cbe":function(e){e.exports=JSON.parse('[{"name":"QQP","description":"The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":40430,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":90.91},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NAACL-devlin2018bert/images/sha256-86185c5af1ece9d2515620d5012bdb3234944f0c71f905d3ecc497e6e6bf7383?context=explore"},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":90.98},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NAACL-devlin2018bert/images/sha256-86185c5af1ece9d2515620d5012bdb3234944f0c71f905d3ecc497e6e6bf7383?context=explore"},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.66},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-yang2019xlnet/images/sha256-c14cee8e6b798cee0890d5da78ef80e50520844131d70dea27ac4cc7262d0a8e?context=explore"},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.79},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-yang2019xlnet/images/sha256-c14cee8e6b798cee0890d5da78ef80e50520844131d70dea27ac4cc7262d0a8e?context=explore"},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":91.41},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-liu2019roberta/images/sha256-bdb919df58fb03196318f5ec55c2be1acbcdee45d05a0e4e70a0fc75666d0d92?context=explore"},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":92.03},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-liu2019roberta/images/sha256-bdb919df58fb03196318f5ec55c2be1acbcdee45d05a0e4e70a0fc75666d0d92?context=explore"},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.73},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"albert-large-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.91},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":92.28},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"distilbert-base-cased","paper_link":"https://arxiv.org/abs/1910.01108","github_link":"https://github.com/huggingface/transformers/tree/master/examples/distillation","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":89.73},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-sanh2019distilbert/images/sha256-2d1c11bc13adb75705eb68b798f03888e268a10064d11b17ed7418a243edf211?context=explore"},{"model_name":"electra-base","paper_link":"https://arxiv.org/abs/2003.10555","github_link":"https://github.com/google-research/electra","paper_name":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","metric":{"Accuracy":91.72},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-clark2020electra/images/sha256-0231b82e48b3e3a81d8132d123a64ec965491933bad011f3b107ed96d563e8f8?context=explore"},{"model_name":"funnel-transformer-medium","paper_link":"https://arxiv.org/abs/2006.03236","github_link":"https://www.github.com/laiguokun/Funnel-Transformer","paper_name":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing","metric":{"Accuracy":92.02},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-dai2020funnel/images/sha256-9b0cf89f368d55c4d611a546054605549a399d3b42f7436270fd64e277d54d36?context=explore"},{"model_name":"spanbert-base-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":90.82},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ACL-joshi2020spanbert/images/sha256-8a51ae80ab9cd4ae31973fa2655b02e12f7e6614116f5e935ff378194b132c7c?context=explore"},{"model_name":"bert_base-two_stage","paper_link":"https://aclanthology.org/2021.eacl-main.8.pdf","github_link":"hhttps://github.com/castorini/berxit","paper_name":"BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression","metric":{"Accuracy":91.05},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ACL-xin2021berxit/images/sha256-18317b8dccebe681ff46528de2e7830455fe62c90a140bc9db9bc740d3fc99b1?context=explore"},{"model_name":"deberta-base","paper_link":"https://arxiv.org/abs/2006.03654","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"Accuracy":91.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-he2020deberta/images/sha256-f13929d4973ec55c907989063699ce7f2bdcb803e91f77bc117e1395ef29dbde?context=explore"}]},{"name":"MRPC","description":" The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":1725,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":84.28},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NAACL-devlin2018bert/images/sha256-86185c5af1ece9d2515620d5012bdb3234944f0c71f905d3ecc497e6e6bf7383?context=explore"},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":82.14},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NAACL-devlin2018bert/images/sha256-86185c5af1ece9d2515620d5012bdb3234944f0c71f905d3ecc497e6e6bf7383?context=explore"},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":84.57},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-yang2019xlnet/images/sha256-c14cee8e6b798cee0890d5da78ef80e50520844131d70dea27ac4cc7262d0a8e?context=explore"},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":87.13},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-yang2019xlnet/images/sha256-c14cee8e6b798cee0890d5da78ef80e50520844131d70dea27ac4cc7262d0a8e?context=explore"},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.24},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-liu2019roberta/images/sha256-bdb919df58fb03196318f5ec55c2be1acbcdee45d05a0e4e70a0fc75666d0d92?context=explore"},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.59},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-liu2019roberta/images/sha256-bdb919df58fb03196318f5ec55c2be1acbcdee45d05a0e4e70a0fc75666d0d92?context=explore"},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":86.02},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"albert-large-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":86.49},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":87.59},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-lan2019albert/images/sha256-f7cfdbc0e680cdf62a70c0d05437b40855b7c15f8f3b741766c1e87cbe5d7ffb?context=explore"},{"model_name":"distilbert-base-cased","paper_link":"https://arxiv.org/abs/1910.01108","github_link":"https://github.com/huggingface/transformers/tree/master/examples/distillation","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":78.49},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-sanh2019distilbert/images/sha256-2d1c11bc13adb75705eb68b798f03888e268a10064d11b17ed7418a243edf211?context=explore"},{"model_name":"electra-base","paper_link":"https://arxiv.org/abs/2003.10555","github_link":"https://github.com/google-research/electra","paper_name":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators","metric":{"Accuracy":86.37},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ICLR-clark2020electra/images/sha256-0231b82e48b3e3a81d8132d123a64ec965491933bad011f3b107ed96d563e8f8?context=explore"},{"model_name":"funnel-transformer-medium","paper_link":"https://arxiv.org/abs/2006.03236","github_link":"https://www.github.com/laiguokun/Funnel-Transformer","paper_name":"Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing","metric":{"Accuracy":87.07},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-NIPS-dai2020funnel/images/sha256-9b0cf89f368d55c4d611a546054605549a399d3b42f7436270fd64e277d54d36?context=explore"},{"model_name":"spanbert-base-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":84.75},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ACL-joshi2020spanbert/images/sha256-8a51ae80ab9cd4ae31973fa2655b02e12f7e6614116f5e935ff378194b132c7c?context=explore"},{"model_name":"bert_base-two_stage","paper_link":"https://aclanthology.org/2021.eacl-main.8.pdf","github_link":"hhttps://github.com/castorini/berxit","paper_name":"BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression","metric":{"Accuracy":83.76},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-ACL-xin2021berxit/images/sha256-18317b8dccebe681ff46528de2e7830455fe62c90a140bc9db9bc740d3fc99b1?context=explore"},{"model_name":"deberta-base","paper_link":"https://arxiv.org/abs/2006.03654","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"Accuracy":88.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/SM-arxiv-he2020deberta/images/sha256-f13929d4973ec55c907989063699ce7f2bdcb803e91f77bc117e1395ef29dbde?context=explore"}]}]')},9702:function(e){e.exports=JSON.parse('[{"name":"SNLI","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":10000,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":88.99},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":89.37},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":89.45},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":89.97},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.56},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":88.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.19},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"Match-lstm","paper_link":"https://arxiv.org/abs/1512.08849","github_link":"https://github.com/shuohangwang/SeqMatchSeq","paper_name":"Learning Natural Language Inference with Lstm","metric":{"Accuracy":83.22},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-wang2015learning/images/sha256-d2e3daa9c02c25d14527f230117ec7627fd74e32727437ae1d53a3799445f5c6?context=explore"},{"model_name":"ESIM","paper_link":"https://arxiv.org/abs/1609.06038","github_link":"https://github.com/lukecq1231/nli","paper_name":"Enhanced LSTM for Natural Language Inference","metric":{"Accuracy":85.34},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ACL-chen2016enhanced/images/sha256-c72669218b439e7b1da4d03cadc84d4ba10808cb27ae8cfea42906ae593c0309?context=explore"},{"model_name":"RE2","paper_link":"https://aclanthology.org/P19-1465/","github_link":"https://github.com/alibaba-edu/simple-effective-text-matching-pytorch","paper_name":"Simple and Effective Text Matching with Richer Alignment Features","metric":{"Accuracy":87.77},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ACL-yang2019simple/images/sha256-adec6437b6f3c3dae5a12bd5320d05186c601575a0e9f1977b857eeb64ac00bc?context=explore"}]},{"name":"MNLI-m","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":9815,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":84.4},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":86.18},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":86.66},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":89.03},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.54},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":84.15},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":89.45},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"spanbert-base-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":85.16},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-TACL-joshi2020spanbert/images/sha256-343fcd3a96880da10857af170c104948485aa5381b36228df16b8937a8277d2d?context=explore"},{"model_name":"spanbert-large-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":87.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-TACL-joshi2020spanbert/images/sha256-343fcd3a96880da10857af170c104948485aa5381b36228df16b8937a8277d2d?context=explore"},{"model_name":"deberta-v2-xxlarge","paper_link":"https://arxiv.org/abs/2006.03654","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"Accuracy":91.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-he2020deberta/images/sha256-ae947d20f7a2226fdec87818ce92c74aa7c241411d4ebc174f4c34c815c1e648?context=explore"}]},{"name":"MNLI-mm","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":9832,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":84.42},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":86.36},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NAACL-devlin2018bert/images/sha256-5f8ab1c300d06a2d319a7fccc4e4ad8272f360832104ce7ef00c04ab75147ee7?context=explore"},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":86.33},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":88.62},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-NIPS-yang2019xlnet/images/sha256-7bd38741e553208f976ed4f5a03461706a33f4b5b6c1b1b37863d893224d0e8e?context=explore"},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.13},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.12},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-arxiv-liu2019roberta/images/sha256-c35cceaa97483a8b664bb031e2eb857caba26a0a5108b0a8fe7dd0c74860fb03?context=explore"},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":84.09},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":89.89},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-lan2019albert/images/sha256-8e64dcc8b81c6235baa59406ea26a9ced03afc7198beb3930e64877f0e90fa8d?context=explore"},{"model_name":"spanbert-base-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":85.49},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-TACL-joshi2020spanbert/images/sha256-343fcd3a96880da10857af170c104948485aa5381b36228df16b8937a8277d2d?context=explore"},{"model_name":"spanbert-large-cased","paper_link":"https://arxiv.org/abs/1907.10529","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":87.13},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-TACL-joshi2020spanbert/images/sha256-343fcd3a96880da10857af170c104948485aa5381b36228df16b8937a8277d2d?context=explore"},{"model_name":"deberta-v2-xxlarge","paper_link":"https://arxiv.org/abs/2006.03654","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"Accuracy":91.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/NLI-ICLR-he2020deberta/images/sha256-ae947d20f7a2226fdec87818ce92c74aa7c241411d4ebc174f4c34c815c1e648?context=explore"}]}]')},"9c0d":function(e){e.exports=JSON.parse('[{"name":"SQuAD1.1","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":87599,"models":[{"model_name":"BiDAF","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/galsang/BiDAF-pytorch","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":66.85,"F1":76.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BiDAF+","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":68.1,"F1":77.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"DrQA","paper_link":"https://arxiv.org/pdf/1704.00051.pdf","github_link":"https://github.com/hitvoice/DrQA","paper_name":"Reading Wikipedia to Answer Open-Domain Questions","metric":{"EM":69.1,"F1":78.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"R-Net","paper_link":"https://www.aclweb.org/anthology/P17-1018.pdf","github_link":"https://github.com/matthew-z/R-net","paper_name":"Gated Self-Matching Networks for Reading Comprehension and Question Answering","metric":{"EM":71,"F1":79.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"FusionNet","paper_link":"https://arxiv.org/pdf/1711.07341.pdf","github_link":"https://github.com/momohuang/FusionNet-NLI","paper_name":"FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION","metric":{"EM":71.7,"F1":80.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"QANet","paper_link":"https://arxiv.org/pdf/1804.09541.pdf","github_link":"https://github.com/BangLiu/QANet-PyTorch","paper_name":"QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION","metric":{"EM":70.3,"F1":80.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BERT","paper_link":"https://arxiv.org/pdf/1810.04805.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"EM":78.9,"F1":86.9},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"ALBERT","paper_link":"https://arxiv.org/pdf/1909.11942.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","metric":{"EM":83.9,"F1":90.8},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/pdf/1910.01108.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"EM":76.2,"F1":85.2},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"XLNet","paper_link":"https://arxiv.org/pdf/1906.08237.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"EM":81.1,"F1":89.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"SRU","paper_link":"https://arxiv.org/pdf/1709.02755.pdf","github_link":"https://github.com/asappresearch/sru","paper_name":"Simple Recurrent Units for Highly Parallelizable Recurrence","metric":{"EM":70.2,"F1":79.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-EMNLP-lei2017simple/images/sha256-e6f582da7d91040a6021037ff0cc4fb2c5d885a4a74878601561ec42e5333764?context=explore"},{"model_name":"FastQA","paper_link":"https://arxiv.org/pdf/1703.04816.pdf","github_link":"https://github.com/uclnlp/jack","paper_name":"Making Neural QA as Simple as Possible but not Simpler","metric":{"EM":70.3,"F1":78.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-COLING-weissenborn2017making/images/sha256-7221c3d4f497b67bef40146db08b7c8f366bcd0ff4a646a2f5572c5e5259ae59?context=explore"},{"model_name":"DeBERTa","paper_link":"https://arxiv.org/pdf/2006.03654.pdf","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"EM":90.1,"F1":95.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-he2020deberta/images/sha256-3f37c654a4ce2480c50230d539770dff5bef5af4f3ceaadb3fc54b0886f44d56?context=explore"},{"model_name":"SpanBERT","paper_link":"https://watermark.silverchair.com/tacl_a_00300.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAs4wggLKBgkqhkiG9w0BBwagggK7MIICtwIBADCCArAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMgYVy0bBufoWyjOccAgEQgIICgcrrQqjxromyESBwbwykux181bgRJyRh7R8Npd1p5fWwpybJLr8QlKyQpt1BUgS4xMmdyvkUkYw0_g2ejr9PVDb3QtMXVivvV9rkmC-tgmCbFNiLs0FUBTo0MprjsmSCYLORNaFclXIMnkzXzE4VwDbxwf3USuPki4KKDp6LSKgJTVCOl7BWQLCqcBDs81GLYHjXFEB7nScPjbur1atBes6QhxGjWWW5sVMDqSuXu2XJaEOmQzwoyqg3rf5UPaVX8-XKJ4phBAQp8TC6M6qhqDgowdwNpMfjaB8bOQoP243tyksmQ2LorSX8-omSw_7RJsC5frZWSfZp4PtgzOQhAHVkM18iorYRlRgnV621k4_4Rrxp5Q5uZ9IKCfKV3JAjO4QsHt6nzKjL91u2JZmU5McMl4ZCEYfTPPmtA-ajxYjeRuhICbk0_lGJ2msJEfea2glViSvxKuVh_Uxn-L3qK4KZsm0ZebnByNB5Zr-prpNwT3xXvi3F564fB6g8hnnwnE3G425N7tzL4o0llqAKjSGj3u9OWJYA4cerEYl2BNw0k-cWbVOuriGaSv_yOZQIKjU0lcWCEZfydbaGHxUjAExyFvS_HZMgf6UDqFm7RC3sMhVQ8kb4F1YYXH3PwBVRKIe3bjpxAJc6pVaeCn4X39pW3lkbLQN36_qVCmaAX0IJaz3TAo01Gvf5T1F_BmhVxu5CffA4w1hRDKW3nwM1XNtCLafq3357u70IYL14Aaja6aQt5tIhHf8Bs3MO2XTlsuTHs_XGc2P6wkzSiU5_aiTBVXDw6qKBo8RnM_zg-1B7ifp8fXS2lxhDWGKMwA2nI8uW8uwpasyv5LuLjimW5fb7","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"EM":88.8,"F1":94.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-TACL-joshi2020spanbert/images/sha256-fe869ed147dc7a7c2c2b75e1b2c50141ffa882e7894e4ff4d337f3b772f763f9?context=explore"},{"model_name":"LUKE","paper_link":"https://arxiv.org/pdf/2010.01057.pdf","github_link":"https://github.com/studio-ousia/luke","paper_name":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","metric":{"EM":89.8,"F1":95},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-EMNLP-yamada2020luke/images/sha256-aafdcef815d2063a22efdc8e580f9327790eddc133eeb7fcbde3c0788e994f01?context=explore"},{"model_name":"RMReader","paper_link":"https://arxiv.org/pdf/1705.02798.pdf","github_link":"https://github.com/HKUST-KnowComp/MnemonicReader","paper_name":"Reinforced Mnemonic Reader for Machine Reading Comprehension","metric":{"EM":78.9,"F1":86.3},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-IJCAI-hu2017reinforced/images/sha256-5a8089fa4cb6e8fb2d15352c550164c0e9780d22697c1124362e002178056769?context=explore"}]},{"name":"SQuAD2.0","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":130319,"models":[{"model_name":"BiDAF","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/galsang/BiDAF-pytorch","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":61.4,"F1":64.5},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BiDAF+ELMo","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":62.57,"F1":65.38},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BiDAF+","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":61.75,"F1":65.01},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BiDAF++ELMo","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":64.8,"F1":67.91},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-seo2016bidirectional/images/sha256-ea6352efdb88ccaee07889079771df4998202d620d7e0280d17b553ad2d268d1?context=explore"},{"model_name":"BERT","paper_link":"https://arxiv.org/pdf/1810.04805.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"EM":72.36,"F1":75.82},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"ALBERT","paper_link":"https://arxiv.org/pdf/1909.11942.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","metric":{"EM":79.39,"F1":82.49},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/pdf/1910.01108.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"EM":66.26,"F1":69.68},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"XLNet","paper_link":"https://arxiv.org/pdf/1906.08237.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"EM":81.01,"F1":85},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-NAACL-devlin2018bert/images/sha256-3e8c286e0a1f526c9632e21755ce4e776c1f8647a3a84572cd3a3ba0a268f12f?context=explore"},{"model_name":"SpanBERT","paper_link":"https://watermark.silverchair.com/tacl_a_00300.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAs4wggLKBgkqhkiG9w0BBwagggK7MIICtwIBADCCArAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMgYVy0bBufoWyjOccAgEQgIICgcrrQqjxromyESBwbwykux181bgRJyRh7R8Npd1p5fWwpybJLr8QlKyQpt1BUgS4xMmdyvkUkYw0_g2ejr9PVDb3QtMXVivvV9rkmC-tgmCbFNiLs0FUBTo0MprjsmSCYLORNaFclXIMnkzXzE4VwDbxwf3USuPki4KKDp6LSKgJTVCOl7BWQLCqcBDs81GLYHjXFEB7nScPjbur1atBes6QhxGjWWW5sVMDqSuXu2XJaEOmQzwoyqg3rf5UPaVX8-XKJ4phBAQp8TC6M6qhqDgowdwNpMfjaB8bOQoP243tyksmQ2LorSX8-omSw_7RJsC5frZWSfZp4PtgzOQhAHVkM18iorYRlRgnV621k4_4Rrxp5Q5uZ9IKCfKV3JAjO4QsHt6nzKjL91u2JZmU5McMl4ZCEYfTPPmtA-ajxYjeRuhICbk0_lGJ2msJEfea2glViSvxKuVh_Uxn-L3qK4KZsm0ZebnByNB5Zr-prpNwT3xXvi3F564fB6g8hnnwnE3G425N7tzL4o0llqAKjSGj3u9OWJYA4cerEYl2BNw0k-cWbVOuriGaSv_yOZQIKjU0lcWCEZfydbaGHxUjAExyFvS_HZMgf6UDqFm7RC3sMhVQ8kb4F1YYXH3PwBVRKIe3bjpxAJc6pVaeCn4X39pW3lkbLQN36_qVCmaAX0IJaz3TAo01Gvf5T1F_BmhVxu5CffA4w1hRDKW3nwM1XNtCLafq3357u70IYL14Aaja6aQt5tIhHf8Bs3MO2XTlsuTHs_XGc2P6wkzSiU5_aiTBVXDw6qKBo8RnM_zg-1B7ifp8fXS2lxhDWGKMwA2nI8uW8uwpasyv5LuLjimW5fb7","github_link":"https://github.com/facebookresearch/SpanBERT","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"EM":85.7,"F1":88.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-TACL-joshi2020spanbert/images/sha256-fe869ed147dc7a7c2c2b75e1b2c50141ffa882e7894e4ff4d337f3b772f763f9?context=explore"},{"model_name":"DeBERTa","paper_link":"https://arxiv.org/pdf/2006.03654.pdf","github_link":"https://github.com/microsoft/DeBERTa","paper_name":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention","metric":{"EM":88,"F1":90.7},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-ICLR-he2020deberta/images/sha256-3f37c654a4ce2480c50230d539770dff5bef5af4f3ceaadb3fc54b0886f44d56?context=explore"},{"model_name":"UNet","paper_link":"https://arxiv.org/pdf/1810.06638.pdf","github_link":"https://github.com/FudanNLP/UNet","paper_name":"U-Net: Machine Reading Comprehension with Unanswerable Questions","metric":{"EM":69.2,"F1":72.6},"dockerhub_link":"https://hub.docker.com/layers/fudannlp/reimplement/MRC-arXiv-sun2018unet/images/sha256-357efd8872859a4a85d222aad4d57e453723da3cf11cac8f0ef69b43a9c29afb?context=explore"}]}]')},a01b:function(e){e.exports=JSON.parse('[{"name":"CMRC2018","description":"The standard CRMC2018 dataset consists of 10,142 training, 3,219 development, and 1,002 test Chinese questions and more than 3000 paragraphs. Our task-specfic transformations are based on CMRC2018 Dev. The dev set of CMRC2018 owns 848 different paragraphs.","available_transformation_type":["domain","ut"],"dataset_size":3219,"models":[{"model_name":"Chinese-BERT-wwm","paper_link":"https://arxiv.org/pdf/1906.08101.pdf","github_link":"https://github.com/ymcui/Chinese-BERT-wwm","dockerhub_link":"","paper_name":"Pre-Training with Whole Word Masking for Chinese BERT","metric":{"EM":60.986}},{"model_name":"ChineseBERT","paper_link":"https://arxiv.org/pdf/2106.16038.pdf","github_link":"https://github.com/ShannonAI/ChineseBert","dockerhub_link":"","paper_name":"ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information","metric":{"EM":62.833}},{"model_name":"CPT","paper_link":"https://arxiv.org/pdf/2109.05729.pdf","github_link":"https://github.com/fastnlp/cpt","dockerhub_link":"","paper_name":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation","metric":{"EM":63.816}},{"model_name":"RoBERTA","paper_link":"https://arxiv.org/pdf/1906.08101.pdf","github_link":"https://github.com/ymcui/Chinese-BERT-wwm","dockerhub_link":"","paper_name":"Pre-Training with Whole Word Masking for Chinese BERT","metric":{"EM":62.54}},{"model_name":"MacBERT","paper_link":"https://arxiv.org/pdf/2004.13922.pdf","github_link":"https://github.com/ymcui/MacBERT","dockerhub_link":"","paper_name":"Revisiting Pre-Trained Models for Chinese Natural Language Processing","metric":{"EM":61.402}}]}]')},a578:function(e){e.exports=JSON.parse('{"name":"CWS","full_name":"Chinese Word Segmentation","description":"Chinese Word Segmentation is the task of segmenting the correct words in a specific context","available_domain":["SwapName","SwapNum","SwapVerb","SwapContraction","SwapSyn","SwapRedup","SwapGene"],"available_ut":[]}')},a70b:function(e){e.exports=JSON.parse('{"name":"TC","full_name":"Text Classification","description":"Text classification is the task of assigning a sentence or document an appropriate category. The categories depend on the chosen dataset and can range from topics. We only chose large-scale dataset.","available_domain":[],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","WordCase-lower","WordCase-title","TwitterType"]}')},aa18:function(e){e.exports=JSON.parse('{"name":"COREF","full_name":"Coreference Resolution","description":"Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.","available_domain":["RndConcat","RndDelete","RndInsert","RndRepeat","RndReplace","RndShuffle"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RevNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","AppendIrr","WordCase-title","WordCase-upper","WordCase-lower","TwitterType"]}')},adbc:function(e){e.exports=JSON.parse('{"name":"NLI","full_name":"Natural Language Inference","description":"Natural language inference is the task of determining whether a \\"hypothesis\\" is true (entailment), false (contradiction), or undetermined (neutral) given a \\"premise\\".","available_domain":["SwapAnt","AddSent","NumWord","Overlap"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType","BackTrans"]}')},ade1:function(e){e.exports=JSON.parse('{"name":"MRCCN","full_name":"Chinese Machine Reading Comprehension","description":"The purpose of chinese machine reading comprehension (CMRC) is to understand the content of a given text and to answer questions about it","available_domain":["ModifyPos","PerturbAnswer","PerturbQuestion"],"available_ut":["AppendIrr","BackTrans","CnNasal","CnSwapSynWordEmbedding","InsertAdv","MLMSuggestion","CnSwapNum","CnSwapNamedEnt","CnSpellingError","CnPunctuation","CnPrejudice","CnDigit2Char","CnHomophones","CnSynonym","CnAntonym"]}')},b18a:function(e){e.exports=JSON.parse('{"name":"ABSA","full_name":"Aspect-Based Sentiment Analysis","description":"Aspect-based sentiment analysis (ABSA) is an advanced sentiment analysis task that aims to classify the sentiment towards a specific aspect.","available_domain":["RevTgt","RevNon","AddDiff"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType"]}')},b27b:function(e){e.exports=JSON.parse('[{"name":"WMT-14(En-De)","description":"This dataset has 4.5M sentence pairs, which is tokenized and split using byte pair encoded (BPE) with 32K(option) merge operations and a shared vocabulary(option) for English and GermanWe use newstest2013 as the validation set and newstest2014 as the test set, which contain 3000 and 3003 sentences, respectively. Models are evaluated based on BLEU.","available_transformation_type":["domain","ut"],"dataset_size":"4.5M","models":[{"model_name":"transformer","paper_link":"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf","paper_name":"Attention is all you need.","github_link":"","metric":{"BLEU":27.27},"dockerhub_link":""},{"model_name":"LSTM-attention","paper_link":"https://nlp.stanford.edu/pubs/emnlp15_attn.pdf","paper_name":"Effective Approaches to Attention-based Neural Machine Translation.","github_link":"","metric":{"BLEU":23.37},"dockerhub_link":""},{"model_name":"CNN","paper_link":"https://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf","paper_name":"Convolutional Sequence to Sequence Learning.","github_link":"","metric":{"BLEU":23.9},"dockerhub_link":""},{"model_name":"scaling","paper_link":"https://aclanthology.org/W18-6301.pdf","paper_name":"Scaling Neural Machine Translation.","github_link":"https://github.com/pytorch/fairseq/blob/main/examples/scaling_nmt/README.md","metric":{"BLEU":28.28},"dockerhub_link":""},{"model_name":"joint_alignment","paper_link":"https://aclanthology.org/D19-1453.pdf","paper_name":"Jointly Learning to Align and Translate with Transformer Models.","github_link":"https://github.com/pytorch/fairseq/blob/main/examples/joint_alignment_translation/README.md","metric":{"BLEU":27.96},"dockerhub_link":""},{"model_name":"Batch-level Selection","paper_link":"https://aclanthology.org/2021.acl-long.504.pdf","paper_name":"Selective Knowledge Distillation for Neural Machine Translation.","github_link":"https://github.com/LeslieOverfitting/selective_distillation","metric":{"BLEU":27.99},"dockerhub_link":""},{"model_name":"Global-level Selection","paper_link":"https://aclanthology.org/2021.acl-long.504.pdf","paper_name":"Selective Knowledge Distillation for Neural Machine Translation.","github_link":"https://github.com/LeslieOverfitting/selective_distillation","metric":{"BLEU":28},"dockerhub_link":""},{"model_name":"VOLT-transformer","paper_link":"https://aclanthology.org/2021.acl-long.571.pdf","paper_name":"Vocabulary Learning via Optimal Transport for Neural Machine Translation.","github_link":"https://github.com/Jingjing-NLP/VOLT","metric":{"BLEU":27.89},"dockerhub_link":""}]}]')},b485:function(e,a,t){var n={"./ABSA/task_description.json":"b18a","./COREF/task_description.json":"aa18","./CWS/task_description.json":"a578","./DP/task_description.json":"eb9a","./DPCN/task_description.json":"d20d","./MRC/task_description.json":"ecf5","./MRCCN/task_description.json":"ade1","./NER/task_description.json":"4ed0","./NERCN/task_description.json":"ce0e","./NLI/task_description.json":"adbc","./NMT/task_description.json":"08f7","./POS/task_description.json":"7b5f","./RE/task_description.json":"bcc6","./SA/task_description.json":"fd07","./SACN/task_description.json":"e6fd","./SM/task_description.json":"7aec","./SMCN/task_description.json":"d2f1","./TC/task_description.json":"a70b","./WSC/task_description.json":"ef12","./WSD/task_description.json":"4a32"};function r(e){var a=i(e);return t(a)}function i(e){if(!t.o(n,e)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return n[e]}r.keys=function(){return Object.keys(n)},r.resolve=i,e.exports=r,r.id="b485"},bcc6:function(e){e.exports=JSON.parse('{"name":"RE","full_name":"Relation Extraction","description":"Relation extraction aims to classify relation types of given entity pairs in text","available_domain":["SwapEnt-LowFreq","SwapEnt-MultiType","SwapEnt-SamEtype","SwapTriplePos-Age","SwapTriplePos-Employee","SwapTriplePos-Birth","InsertClause"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","TwitterType"]}')},ce0e:function(e){e.exports=JSON.parse('{"name":"NERCN","full_name":"Chinese Named Entity Recognition","description":"Chinese named entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. from Chinese text","available_domain":["OOV","SwapLonger","EntTypos","ConcatSent"],"available_ut":["CnSwapSynWordEmbedding","CnSpellingError","CnHomophones","CnSynonym","CnAntonym"]}')},ce4f:function(e){e.exports=JSON.parse('[{"name":"IMDB","description":"The IMDb dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score  4 out of 10, and a positive review has a score  7 out of 10. No more than 30 reviews are included per movie. Models are evaluated based on Accuracy.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":25000,"models":[{"model_name":"XLNET","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":96.17},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-NeurIPS-yang2020xlnet/images/sha256-9e9dd84f4f888acfdd99ddc61a0bf2ceb4bef001833a58caab7240c20fc5d1e9?context=explore"},{"model_name":"BERT-Large-ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":95.3},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-CCL-sun2020finetune/images/sha256-67ae76508c42d6726aea3c16245ab036175dab8155fe51752572981234e78423?context=explore"},{"model_name":"ULMFIT","paper_link":"https://arxiv.org/abs/1801.06146","github_link":"https://github.com/fastai/fastai/blob/54a9e3cf4fd0fa11fc2453a5389cc9263f6f0d77/examples/ULMFit.ipynb","paper_name":"Universal Language Model Fine-tuning for Text Classification","metric":{"Accuracy":94.6},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ACL-howard2018universal/images/sha256-f39e2aa11c0b5fa587170294332334e51ad73f578d5548c7695ad0f9d9d51d7a?context=explore"},{"model_name":"oh-lstm","paper_name":"Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings","metric":{"Accuracy":93.86},"paper_link":"https://arxiv.org/abs/1602.02373","github_link":"http://riejohnson.com/cnn_download.html","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ICML-johnson2016supervised/images/sha256-81c3f41c17006788c1c206bb1422d8764636e95124f93ad83bdb6f8a2783e94f?context=explore"},{"model_name":"adv","paper_name":"Adversarial Training Methods for Semi-Supervised Text Classification","metric":{"Accuracy":93.26},"paper_link":"https://arxiv.org/abs/1605.07725","github_link":"https://github.com/tensorflow/models/tree/master/research/adversarial_text","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ICLR-miyato2016adversarial/images/sha256-ab81528cbfe66c01da4630fbc4435d54dc0ee0363eb407f6bbd8e389a5b73105?context=explore"},{"model_name":"ssl","paper_link":"https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_46.pdf","paper_name":"Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function","metric":{"Accuracy":93.23},"github_link":"https://github.com/DevSinghSachan/ssl_text_classification","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-AAAI-Sachan_2019/images/sha256-42ad844d5964352d97f6777dd8af66511177b3f0afaae9d5f6b09adc829b5695?context=explore"},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/abs/1910.01108","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":92.2},"github_link":"https://github.com/huggingface/transformers","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-AAAI-Sachan_2019/images/sha256-42ad844d5964352d97f6777dd8af66511177b3f0afaae9d5f6b09adc829b5695?context=explore"},{"model_name":"FastText","paper_link":"https://arxiv.org/abs/1910.01108","paper_name":"Bag of Tricks for Efficient Text Classification","metric":{"Accuracy":86.3},"github_link":"https://github.com/facebookresearch/fastText","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-EACL-joulin2016bag/images/sha256-bdac8632886c0b8f3064c1b552a69c965ba8ce3713b64db1d5fe2493b5522eb3?context=explore"},{"model_name":"SRNN","paper_link":"https://arxiv.org/abs/1807.02291","paper_name":"Sliced Recurrent Neural Networks","metric":{"Accuracy":89.13},"github_link":"https://github.com/zepingyu0512/srnn","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-COLING-yu2018sliced/images/sha256-d3e82e0a08fe49d03b3b4598fc8cb6ebf8996cc919d32eb5905932e90559d2c6?context=explore"},{"model_name":"BPT","paper_link":"https://arxiv.org/abs/1911.04070v1","paper_name":"BP-Transformer: Modelling Long-Range Context via Binary Partitioning","metric":{"Accuracy":91.28},"github_link":"https://github.com/yzh119/BPT","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-arxiv-ye2019bp/images/sha256-f834a28447ed164e35f42aa69af96092bb8d3d4816933978ef2f0cd9a8a48dd5?context=explore"},{"model_name":"DAN","paper_link":"https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf","paper_name":"Deep Unordered Composition Rivals Syntactic Methods for Text Classification","metric":{"Accuracy":88.8},"github_link":"http://github.com/miyyer/dan","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ACL-iyyer2015deep/images/sha256-a7bcab2706cba6f5dd0b92d8e0ca953e36f3e1fb095926f51519db18fb24af3a?context=explore"}]},{"name":"Yelp-Binary","description":"The Yelp Review dataset consists of more than 500,000 Yelp reviews. There is both a binary and a fine-grained (five-class) version of the dataset. Models are evaluated based on error (1 - Accuracy; lower is better).","available_transformation_type":["ut"],"models":[{"model_name":"XLNET","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":97.83},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-NeurIPS-yang2020xlnet/images/sha256-9e9dd84f4f888acfdd99ddc61a0bf2ceb4bef001833a58caab7240c20fc5d1e9?context=explore"},{"model_name":"BERT-Large-ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":98.06},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-CCL-sun2020finetune/images/sha256-67ae76508c42d6726aea3c16245ab036175dab8155fe51752572981234e78423?context=explore"},{"model_name":"ULMFIT","paper_link":"https://arxiv.org/abs/1801.06146","github_link":"https://github.com/fastai/fastai/blob/54a9e3cf4fd0fa11fc2453a5389cc9263f6f0d77/examples/ULMFit.ipynb","paper_name":"Universal Language Model Fine-tuning for Text Classification","metric":{"Accuracy":97.57},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ACL-howard2018universal/images/sha256-f39e2aa11c0b5fa587170294332334e51ad73f578d5548c7695ad0f9d9d51d7a?context=explore"},{"model_name":"dpcnn","paper_name":"Deep Pyramid Convolutional Neural Networks for Text Categorization","metric":{"Accuracy":97.34},"paper_link":"https://www.aclweb.org/anthology/P17-1052/","github_link":"http://riejohnson.com/cnn_download.html","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ICML-johnson2016supervised/images/sha256-81c3f41c17006788c1c206bb1422d8764636e95124f93ad83bdb6f8a2783e94f?context=explore"},{"model_name":"adv","paper_name":"Adversarial Training Methods for Semi-Supervised Text Classification","metric":{"Accuracy":96.79},"paper_link":"https://arxiv.org/abs/1605.07725","github_link":"https://github.com/tensorflow/models/tree/master/research/adversarial_text","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-ICLR-miyato2016adversarial/images/sha256-ab81528cbfe66c01da4630fbc4435d54dc0ee0363eb407f6bbd8e389a5b73105?context=explore"},{"model_name":"ssl","paper_link":"https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_46.pdf","github_link":"https://github.com/DevSinghSachan/ssl_text_classification","paper_name":"Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function","metric":{"Accuracy":97.57},"dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-AAAI-Sachan_2019/images/sha256-42ad844d5964352d97f6777dd8af66511177b3f0afaae9d5f6b09adc829b5695?context=explore"},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/abs/1910.01108","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":92.2},"github_link":"https://github.com/huggingface/transformers","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-AAAI-Sachan_2019/images/sha256-42ad844d5964352d97f6777dd8af66511177b3f0afaae9d5f6b09adc829b5695?context=explore"},{"model_name":"FastText","paper_link":"https://arxiv.org/abs/1910.01108","paper_name":"Bag of Tricks for Efficient Text Classification","metric":{"Accuracy":93.4},"github_link":"https://github.com/facebookresearch/fastText","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-EACL-joulin2016bag/images/sha256-bdac8632886c0b8f3064c1b552a69c965ba8ce3713b64db1d5fe2493b5522eb3?context=explore"},{"model_name":"SRNN","paper_link":"https://arxiv.org/abs/1910.01108","paper_name":"Sliced Recurrent Neural Networks","metric":{"Accuracy":96.13},"github_link":"https://github.com/zepingyu0512/srnn","dockerhub_link":"https://registry.hub.docker.com/layers/fudannlp/reimplement/SA-COLING-yu2018sliced/images/sha256-d3e82e0a08fe49d03b3b4598fc8cb6ebf8996cc919d32eb5905932e90559d2c6?context=explore"}]}]')},d20d:function(e){e.exports=JSON.parse('{"name":"DPCN","full_name":"Chinese Dependency Parsing","description":"Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and defines the relationships between head words and words, which modify those heads.","available_domain":[],"available_ut":["CnNasal","CnSwapSynWordEmbedding","CnSwapNum","CnSpellingError","CnDigit2Char","CnHomophones","CnSynonym","CnAntonym"]}')},d2f1:function(e){e.exports=JSON.parse('{"name":"SMCN","full_name":"Chinese Semantic Matching","description":"This task need model to identify whether two sentences express the same meaning.","available_domain":["SwapWord","SwapNum","Overlap"],"available_ut":["AppendIrr","BackTrans","CnSwapSynWordEmbedding","InsertAdv","CnSwapNum","CnSwapNamedEnt","CnSpellingError","CnPunctuation","CnPrejudice","CnDigit2Char","CnHomophones","CnSynonym"]}')},d856:function(e){e.exports=JSON.parse('[{"name":"Weibo","description":"Named Entity Recognition (NER) for Chinese Social Media (Weibo). This dataset contains messages selected from Weibo and annotated according to the DEFT ERE annotation guidelines. Annotations include both name and nominal mentions. The corpus contains 1,890 messages sampled from Weibo between November 2013 and December 2014.","available_transformation_type":["domain","ut"],"dataset_size":1890,"models":[{"model_name":"LGN","paper_link":"https://aclanthology.org/D19-1096.pdf","github_link":"https://github.com/RowitZou/LGN","dockerhub_link":"","paper_name":"A Lexicon-Based Graph Neural Network for Chinese NER","metric":{"F1":60.21}},{"model_name":"SoftLexicon","paper_link":"https://arxiv.org/pdf/1908.05969.pdf","github_link":"https://github.com/v-mipeng/LexiconAugmentedNER","dockerhub_link":"","paper_name":"Simplify the Usage of Lexicon in Chinese NER","metric":{"F1":70.5}},{"model_name":"SANER","paper_link":"https://arxiv.org/pdf/2010.15458.pdf","github_link":"https://github.com/cuhksz-nlp/SANER","dockerhub_link":"","paper_name":"Named Entity Recognition for Social Media Texts with Semantic Augmentation","metric":{"F1":67.38}}]},{"name":"OntoNotes 4","description":"A large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in Chinese language","available_transformation_type":["domain","ut"],"dataset_size":15724,"models":[{"model_name":"LGN","paper_link":"https://aclanthology.org/D19-1096.pdf","github_link":"https://github.com/RowitZou/LGN","dockerhub_link":"","paper_name":"A Lexicon-Based Graph Neural Network for Chinese NER","metric":{"F1":74.89}},{"model_name":"SoftLexicon","paper_link":"https://arxiv.org/pdf/1908.05969.pdf","github_link":"https://github.com/v-mipeng/LexiconAugmentedNER","dockerhub_link":"","paper_name":"Simplify the Usage of Lexicon in Chinese NER","metric":{"F1":82.81}},{"model_name":"AESINER","paper_link":"https://arxiv.org/pdf/2010.15466.pdf","github_link":"https://github.com/cuhksz-nlp/AESINER","dockerhub_link":"","paper_name":"Improving Named Entity Recognition with Attentive Ensemble of Syntactic Information","metric":{"F1":81.05}}]},{"name":"MSRA","description":"Provided by Microsoft Research Asia, A set of manually annotated Chinese named entity recognition data and specifications for training and testing a Chinese named entity recognition system for research purposes. ","available_transformation_type":["domain","ut"],"dataset_size":50729,"models":[{"model_name":"LGN","paper_link":"https://aclanthology.org/D19-1096.pdf","github_link":"https://github.com/RowitZou/LGN","dockerhub_link":"","paper_name":"A Lexicon-Based Graph Neural Network for Chinese NER","metric":{"F1":93.46}},{"model_name":"SoftLexicon","paper_link":"https://arxiv.org/pdf/1908.05969.pdf","github_link":"https://github.com/v-mipeng/LexiconAugmentedNER","dockerhub_link":"","paper_name":"Simplify the Usage of Lexicon in Chinese NER","metric":{"F1":95.42}}]}]')},e6fd:function(e){e.exports=JSON.parse('{"name":"SACN","full_name":"Chinese Sentiment Analysis","description":"Chinese Sentiment analysis is the task of classifying the polarity of a given text.","available_domain":["SentenceOrderSwap","ExtentAdjust","CNDoubleDenial"],"available_ut":["AppendIrr","CnSwapSynWordEmbedding","CnSwapNum","CnSwapNamedEnt","CnSpellingError","CnPunctuation","CnHomophones","CnSynonym"]}')},eb9a:function(e){e.exports=JSON.parse('{"name":"DP","full_name":"Dependency Parsing","description":"Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and defines the relationships between head words and words, which modify those heads.","available_domain":["AddSubTree","DeleteSubTree"],"available_ut":["Contraction","InsertAdv","Keyboard","MLMSuggestion","Ocr","Prejudice-Name-woman","ReverseNeg","SwapAnt","SwapNum","SwapSyn-WordNet","Tense","TwitterType","Typos","WordCase-lower","WordCase-title","WordCase-upper"]}')},ecf5:function(e){e.exports=JSON.parse('{"name":"MRC","full_name":"Machine Reading Comprehension","description":"Machine Reading Comprehension is the task is to read and comprehend a given text passage, and then answer questions based on it.","available_domain":["ModifyPos","AddSentDiverse","PerturbAnswer","PerturbQuestion"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","Contraction","Tense","InsertAdv","AppendIrr","WordCase","TwitterType","Prejudice"]}')},ef12:function(e){e.exports=JSON.parse('{"name":"WSC","full_name":"The Winograd Schema Challenge","description":"The Winograd Schema Challenge (WSC) is a pronoun  resolution  task, which the main part of each WSC problem is a set of sentences containing a pronoun and requires the use of world knowledge and reasoning for its resolution. ","available_domain":["InsertRelativeClause","SwapGender","SwapNames","SwitchVoice"],"available_ut":["Tense","WordCase","Ocr","SwapSynWordNet","Keyboard","TwitterType","SpellingError","MLMSuggestion"]}')},fd07:function(e){e.exports=JSON.parse('{"name":"SA","full_name":"Sentiment Analysis","description":"Sentiment analysis is the task of classifying the polarity of a given text.","available_domain":["SwapSpecialEnt-Movie","SwapSpecialEnt-Person","AddSum-Movie","AddSum-Person","DoubleDenial"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","WordCase-lower","WordCase-title","TwitterType"]}')}}]);