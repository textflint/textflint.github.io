(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-390fed5d"],{"0550":function(e){e.exports=JSON.parse('[{"name":"Yelp-Review","description":"Millions of document level text collected from yelp. There are five classes of dataset. The classes represent the user\'s rating of the store.","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":75.8}},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":73.25}},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":72.1}},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":62.05}},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":58.95}},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":66.43}}]},{"name":"IMDB_Large","description":"Millions of movie reviews from IMDB, Choose the most popular reviews from 20 movie categories. There are ten classes of dataset. The classes represent the user\'s rating of the movie","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":53.47}},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":51.53}},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":47.65}},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":47.83}},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":43.06}},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":45.33}}]},{"name":"Amazon-Review","description":"Five million product reviews from Amazon, There are five classes of dataset. The classes represent the user\'s rating of the product","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT-Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":75.8}},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Accuracy":73.25}},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Accuracy":72.1}},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Accuracy":62.05}},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Accuracy":58.95}},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Accuracy":66.43}}]},{"name":"Reuters","description":"Reuters rcv1 and rcv2 800 000 news data, multi label classification, a total of 103 categories","available_transformation_type":["ut","ut_ut"],"models":[{"model_name":"BERT_Large+ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Micro-F1":90.08}},{"model_name":"LSTM","paper_link":"https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735","github_link":"","paper_name":"Long Short-term Memory","metric":{"Micro-F1":88.41}},{"model_name":"HLSTM","paper_link":"https://www.aclweb.org/anthology/N16-1174.pdf","github_link":"","paper_name":"Hierarchical Attention Networks for Document Classificatio","metric":{"Micro-F1":86.85}},{"model_name":"CNN","paper_link":"https://arxiv.org/abs/1408.5882","github_link":"","paper_name":"Convolutional Neural Networks for Sentence Classification","metric":{"Micro-F1":82.49}},{"model_name":"HCNN","paper_link":"","github_link":"","paper_name":"","metric":{"Micro-F1":78.32}},{"model_name":"Transformer","paper_link":"https://arxiv.org/abs/1706.03762","github_link":"","paper_name":"Attention Is All You Need","metric":{"Micro-F1":86.15}}]}]')},"0c12":function(e){e.exports=JSON.parse('[{"name":"CoNLL 2003","description":"The CoNLL 2003 NER task consists of newswire text from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). Models are evaluated based on span-based F1 on the test set.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":3684,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":88.48}},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":90.59}},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":90.88}},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":91.42}},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":90.4}},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":91.53}},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":91.68}},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":91.14}},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":92.25}},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":91.32}}]},{"name":"OntoNotes v5","description":"OntoNotes Release 5.0 iis comprised of 1,745k English, 900k Chinese, and 300k Arabic text data collected from a range of sources including telephone conversations, newswire, talkshows, broadcast news, broadcast conversation, and online blogs. Entities are annotated with labels such as PERSON, ORGANIZATION, and LOCATION among others.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":8469,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":82.31}},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":85.95}},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":87.92}},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":88.85}},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":86.85}},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":88.12}},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":87.84}},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":85.62}},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":86.91}},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":85.57}}]},{"name":"ACE 2005","description":"ACE 2005 Multilingual Training Corpus was developed by the Linguistic Data Consortium (LDC) and contains approximately 1,800 files of mixed genre text in English, Arabic, and Chinese annotated for entities, relations, and events.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":2050,"models":[{"model_name":"BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N16-1030/","github_link":"https://github.com/achernodub/targer","paper_name":"Neural Architectures for Named Entity Recognition","metric":{"F1":82.94}},{"model_name":"BiLSTM-CNN-CRF","paper_link":"https://www.aclweb.org/anthology/P16-1101","github_link":"https://github.com/achernodub/targer","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"F1":82.08}},{"model_name":"LM-LSTM-CRF","paper_link":"https://arxiv.org/pdf/1709.04109v4.pdf","github_link":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF","paper_name":"Empower Sequence Labeling with Task-Aware Neural Language Model","metric":{"F1":85.14}},{"model_name":"BERT-base(cased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":87.27}},{"model_name":"BERT-base(uncased)","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/kamalkraj/BERT-NER","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":88.75}},{"model_name":"TENER","paper_link":"https://arxiv.org/pdf/1911.04474v3.pdf","github_link":"https://github.com/fastnlp/TENER","paper_name":"TENER: Adapting Transformer Encoder for Named Entity Recognition","metric":{"F1":84.19}},{"model_name":"GRN","paper_link":"https://arxiv.org/pdf/1907.05611v2.pdf","github_link":"https://github.com/HuiChen24/NER-GRN","paper_name":"GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition","metric":{"F1":84.91}},{"model_name":"BiLSTM-CRF+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-1202/","github_link":"https://github.com/flairNLP/flair","paper_name":"Deep contextualized word representations","metric":{"F1":85.42}},{"model_name":"flair embeddings","paper_link":"https://www.aclweb.org/anthology/C18-1139.pdf","github_link":"https://github.com/flairNLP/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"F1":85.7}},{"model_name":"Flair embeddings + Pooling","paper_link":"https://www.aclweb.org/anthology/N19-1078/","github_link":"https://github.com/flairNLP/flair","paper_name":"Pooled Contextualized Embeddings for Named Entity Recognition","metric":{"F1":84.65}}]}]')},"0f1b":function(e){e.exports=JSON.parse('[{"name":"WSJ","description":"A standard dataset for POS tagging is the Wall Street Journal (WSJ) portion of the Penn Treebank, containing 45 different POS tags. Sections 0-18 are used for training, sections 19-21 for development, and sections 22-24 for testing.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":5463,"models":[{"model_name":"CRF++","paper_link":"http://www.cis.upenn.edu/~pereira/papers/crf.pdf","github_link":"https://taku910.github.io/crfpp/","paper_name":"Conditional random fields: Probabilistic models for segmenting and labeling sequence data","metric":{"Accuracy":95}},{"model_name":"CNN-BILSTM-CRF","paper_link":"https://www.aclweb.org/old_anthology/P/P16/P16-1101.pdf","github_link":"https://github.com/XuezheMax/NeuroNLP2","paper_name":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","metric":{"Accuracy":97.54}},{"model_name":"BiLSTM-LAN","paper_link":"https://www.aclweb.org/anthology/D19-1422.pdf","github_link":"https://github.com/Nealcly/LAN","paper_name":"Hierarchically-Refined Label Attention Network for Sequence Labeling","metric":{"Accuracy":97.51}},{"model_name":"UANet","paper_link":"https://www.aclweb.org/anthology/2020.emnlp-main.181.pdf","github_link":"https://github.com/jiacheng-ye/UANet","paper_name":"Uncertainty-Aware Label Refinement for Sequence Labeling","metric":{"Accuracy":97.66}},{"model_name":"ELMo-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N18-1202.pdf","github_link":"https://github.com/allenai/allennlp","paper_name":"Deep contextualized word representations","metric":{"Accuracy":97.75}},{"model_name":"Flair-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/C18-1139/","github_link":"https://github.com/zalandoresearch/flair","paper_name":"Contextual String Embeddings for Sequence Labeling","metric":{"Accuracy":97.75}},{"model_name":"BERT-BiLSTM-CRF","paper_link":"https://www.aclweb.org/anthology/N19-1423.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":97.75}}]}]')},"23b9":function(e){e.exports=JSON.parse('[{"name":"PennTreebank","description":"The Penn Treebank Project annotates naturally-occuring text for linguistic structure.","available_transformation_type":["domain","ut","domain_domain","domain_ut"],"dataset_size":2416,"models":[{"model_name":"HPSG-cws","paper_link":"https://arxiv.org/abs/1907.02684","github_link":"https://github.com/DoodleJZ/HPSG-Neural-Parser","paper_name":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank","metric":{"LAS":94.68}},{"model_name":"HPSG-bert","paper_link":"https://arxiv.org/abs/1907.02684","github_link":"https://github.com/DoodleJZ/HPSG-Neural-Parser","paper_name":"Head-Driven Phrase Structure Grammar Parsing on Penn Treebank","metric":{"LAS":95.43}},{"model_name":"L2R","paper_link":"https://arxiv.org/abs/1903.08445","github_link":"https://github.com/danifg/Left2Right-Pointer-Parser","paper_name":"Left-to-Right Dependency Parsing with Pointer Networks","metric":{"LAS":94.43}},{"model_name":"twpipe","paper_link":"https://arxiv.org/abs/1805.11224","github_link":"https://github.com/Oneplus/twpipe","paper_name":"Distilling Knowledge for Search-based Structured Prediction","metric":{"LAS":92.14}},{"model_name":"jPTDP","paper_link":"http://www.aclweb.org/anthology/K18-2008","github_link":"https://github.com/datquocnguyen/jPTDP","paper_name":"An improved neural network model for joint POS tagging and dependency parsing","metric":{"LAS":92.87}},{"model_name":"LAL","paper_link":"https://arxiv.org/abs/1911.03875","github_link":"https://github.com/KhalilMrini/LAL-Parser","paper_name":"Rethinking Self-Attention: Towards Interpretability in Neural Parsing","metric":{"LAS":96.26}},{"model_name":"CRF","paper_link":"https://www.aclweb.org/anthology/2020.acl-main.302","github_link":"https://github.com/yzhangcs/parser","paper_name":"Efficient Second-Order TreeCRF for Neural Dependency Parsing","metric":{"LAS":94.49}}]}]')},"314d":function(e){e.exports=JSON.parse('[{"name":"Tacred","description":"Tacred is a relation extraction dataset with around 15,000 sentences","available_transformation_type":["doman","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"models":[{"model_name":"LSTM-ATT","paper_link":"https://www.aclweb.org/anthology/D17-1004.pdf","github_link":"https://github.com/yuhaozhang/tacred-relation","paper_name":"Position-aware Attention and Supervised Data Improve Slot Filling","metric":{"F1":65.4}},{"model_name":"GCN","paper_link":"https://www.aclweb.org/anthology/D18-1244.pdf","github_link":"https://github.com/qipeng/gcn-over-pruned-trees","paper_name":"Graph Convolution over Pruned Dependency Trees Improves Relation Extraction","metric":{"F1":62.03}},{"model_name":"AGGCN","paper_link":"https://www.aclweb.org/anthology/P19-1024.pdf","github_link":"https://github.com/Cartus/AGGCN","paper_name":"Attention Guided Graph Convolutional Networks for Relation Extraction","metric":{"F1":67.41}},{"model_name":"BERT-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":"68.01"}},{"model_name":"BERT-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"F1":"67.73"}}]},{"name":"NYT","description":"NYT is a distantly supervised relation extraction dataset with around 172,000 sentences","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"models":[{"model_name":"PCNN-ATT","paper_link":"https://www.aclweb.org/anthology/P16-1200.pdf","github_link":"https://github.com/thunlp/OpenNRE","paper_name":"Neural Relation Extraction with Selective Attention over Instances","metric":{"AUC":0.3366,"F1":0.412}},{"model_name":"PCNN+ATT RA+BAG ATT","paper_link":"https://arxiv.org/abs/1904.00143","github_link":"https://github.com/ZhixiuYe/Intra-Bag-and-Inter-Bag-Attentions","paper_name":"Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions","metric":{"AUC":0.4212,"F1":0.4555}},{"model_name":"PCNN","paper_link":"https://www.aclweb.org/anthology/D15-1203.pdf","github_link":"https://github.com/thunlp/OpenNRE","paper_name":"Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks","metric":{"AUC":0.3463,"F1":0.4087}},{"model_name":"SeG","paper_link":"https://arxiv.org/pdf/1911.11899.pdf","github_link":"https://github.com/tmliang/SeG","paper_name":"Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction","metric":{"AUC":0.4575,"F1":0.4927}},{"model_name":"DISTRE","paper_link":"https://www.aclweb.org/anthology/P19-1134","github_link":"https://github.com/DFKI-NLP/DISTRE","paper_name":"Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction","metric":{"AUC":0.4225,"F1":0.4864}}]}]')},"4ed0":function(e){e.exports=JSON.parse('{"name":"NER","full_name":"Named Entity Recognition","description":"Named entity recognition is the task of tagging entities in text with their corresponding type.","available_domain":["ConcatSent","CrossCategory","EntTypos","OOV","SwapLonger"],"available_ut":["RevNeg","AddRmvPunc","AppendIrr","Contraction","InsertAdv","Keyboard","MLMSuggestion","Ocr","SpellingError","SwapAnt-WordNet","SwapNum","SwapSyn-WordEmbedding","SwapSyn-WordNet","Tense","TwitterType","Typos-random","WordCase-lower","WordCase-title","WordCase-upper","WordCase"]}')},6683:function(e){e.exports=JSON.parse('[{"name":"Ontonotes","description":"OntoNotes Release 5.0","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"models":[{"model_name":"c2f+ELMo","paper_link":"https://www.aclweb.org/anthology/N18-2108","github_link":"https://github.com/kentonl/e2e-coref","paper_name":"Higher-order Coreference Resolution with Coarse-to-fine Inference","metric":{"Accuracy":72.39}},{"model_name":"e2e+ELMo","paper_link":"https://www.aclweb.org/anthology/D17-1018/","github_link":"https://github.com/kentonl/e2e-coref/releases/tag/e2e","paper_name":"End-to-end Neural Coreference Resolution","metric":{"Accuracy":67.23}},{"model_name":"c2f+BERT","paper_link":"https://www.aclweb.org/anthology/D19-1588","github_link":"https://github.com/mandarjoshi90/coref","paper_name":"BERT for Coreference Resolution: Baselines and Analysis","metric":{"Accuracy":74.74}},{"model_name":"c2f+SpanBERT","paper_link":"https://www.aclweb.org/anthology/2020.tacl-1.5","github_link":"https://github.com/mandarjoshi90/coref","paper_name":"SpanBERT: Improving Pre-training by Representing and Predicting Spans","metric":{"Accuracy":77.41}}]}]')},7262:function(e){e.exports=JSON.parse('[{"name":"SemEval2014-Restaurant","description":"The standard SemEval2014-Restaurant dataset consists of 3,452 training, 150 validation, and 1,120 test English sentences from the restaurant reviews. Our task-specfic transformations are based on SemEval2014-Restaurant-TOWE, which provides opinion words and their position. The test set of SemEval2014-Restaurant-TOWE owns 492 different sentences (847 aspect terms).","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":1120,"models":[{"model_name":"LCF-BERT","paper_link":"https://www.researchgate.net/publication/335238076_LCF_A_Local_Context_Focus_Mechanism_for_Aspect-Based_Sentiment_Classification","github_link":"https://github.com/yangheng95/LC-ABSA","paper_name":"LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification","metric":{"Accuracy":84.82,"Macro-F1":76.99}},{"model_name":"BERT+ASPECT","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/bert_spc.py","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":85.18,"Macro-F1":77.43}},{"model_name":"BERT-BASE","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":79.02,"Macro-F1":69.17}},{"model_name":"MGAN","paper_link":"https://www.aclweb.org/anthology/D18-1380/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/mgan.py","paper_name":"Multi-grained Attention Network for Aspect-Level Sentiment Classification","metric":{"Accuracy":79.11,"Macro-F1":67.23}},{"model_name":"TNet","paper_link":"https://arxiv.org/pdf/1805.01086","github_link":"https://github.com/lixin4ever/TNet","paper_name":"Transformation Networks for Target-Oriented Sentiment Classification","metric":{"Accuracy":78.93,"Macro-F1":67.2}},{"model_name":"IAN","paper_link":"https://arxiv.org/pdf/1709.00893","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/ian.py","paper_name":"Interactive Attention Networks for Aspect-Level Sentiment Classification","metric":{"Accuracy":76.43,"Macro-F1":63.78}},{"model_name":"MemNet","paper_link":"https://arxiv.org/pdf/1605.08900","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/memnet.py","paper_name":"Aspect Level Sentiment Classification with Deep Memory Network","metric":{"Accuracy":75.71,"Macro-F1":62.23}},{"model_name":"ATAE-LSTM","paper_link":"https://www.aclweb.org/anthology/D16-1058/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/atae_lstm.py","paper_name":"Attention-based lstm for aspect-level sentiment classification","metric":{"Accuracy":77.41,"Macro-F1":63.28}},{"model_name":"TD-LSTM","paper_link":"https://arxiv.org/pdf/1512.01100","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/td_lstm.py","paper_name":"Effective LSTMs for Target-Dependent Sentiment Classification","metric":{"Accuracy":77.59,"Macro-F1":63.28}},{"model_name":"LSTM","paper_link":"https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lstm.py","paper_name":"Long short-term memory","metric":{"Accuracy":76.34,"Macro-F1":63.66}}]},{"name":"SemEval2014-Laptop","description":"The SemEval2014-Laptop dataset consists of 2,163 training, 150 validation, and 638 test English sentences extracted from customer reviews of laptops. Our task-specfic transformations are based on SemEval2014-Laptop-TOWE, which provides opinion words and their position. The test set of SemEval2014-Laptop-TOWE owns 331 different sentences (446 aspect terms).","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":638,"models":[{"model_name":"LCF-BERT","paper_link":"https://www.researchgate.net/publication/335238076_LCF_A_Local_Context_Focus_Mechanism_for_Aspect-Based_Sentiment_Classification","github_link":"https://github.com/yangheng95/LC-ABSA","paper_name":"LCF: A Local Context Focus Mechanism for Aspect-Based Sentiment Classification","metric":{"Accuracy":80.72,"Macro-F1":77.5}},{"model_name":"BERT+ASPECT","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/bert_spc.py","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":76.02,"Macro-F1":71.03}},{"model_name":"BERT-BASE","paper_link":"https://www.aclweb.org/anthology/N19-1423/","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":78.21,"Macro-F1":73.16}},{"model_name":"MGAN","paper_link":"https://www.aclweb.org/anthology/D18-1380/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/mgan.py","paper_name":"Multi-grained Attention Network for Aspect-Level Sentiment Classification","metric":{"Accuracy":72.41,"Macro-F1":65.62}},{"model_name":"TNet","paper_link":"https://arxiv.org/pdf/1805.01086","github_link":"https://github.com/lixin4ever/TNet","paper_name":"Transformation Networks for Target-Oriented Sentiment Classification","metric":{"Accuracy":71.16,"Macro-F1":65.9}},{"model_name":"IAN","paper_link":"https://arxiv.org/pdf/1709.00893","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/ian.py","paper_name":"Interactive Attention Networks for Aspect-Level Sentiment Classification","metric":{"Accuracy":70.53,"Macro-F1":63.48}},{"model_name":"MemNet","paper_link":"https://arxiv.org/pdf/1605.08900","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/memnet.py","paper_name":"Aspect Level Sentiment Classification with Deep Memory Network","metric":{"Accuracy":64.11,"Macro-F1":55.87}},{"model_name":"ATAE-LSTM","paper_link":"https://www.aclweb.org/anthology/D16-1058/","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/atae_lstm.py","paper_name":"Attention-based lstm for aspect-level sentiment classification","metric":{"Accuracy":69.91,"Macro-F1":64.08}},{"model_name":"TD-LSTM","paper_link":"https://arxiv.org/pdf/1512.01100","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/td_lstm.py","paper_name":"Effective LSTMs for Target-Dependent Sentiment Classification","metric":{"Accuracy":68.34,"Macro-F1":63.22}},{"model_name":"LSTM","paper_link":"https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory","github_link":"https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lstm.py","paper_name":"Long short-term memory","metric":{"Accuracy":70.38,"Macro-F1":62.59}}]}]')},"7aec":function(e){e.exports=JSON.parse('{"name":"SM","full_name":"Semantic Matching","description":"This task need model to identify whether two sentences express the same meaning.","available_domain":["SwapWord","SwapNum","Overlap"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RmvPunc","AddNeg","RmvNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType","BackTrans","Prejudge"]}')},"7b5f":function(e){e.exports=JSON.parse('{"name":"POS","full_name":"Part-of-speech tagging","description":"Part-of-speech tagging (POS tagging) is the task of tagging a word in a text with its part of speech. A part of speech is a category of words with similar grammatical properties. Common English parts of speech are noun, verb, adjective, adverb, pronoun, preposition, conjunction, etc.","available_domain":["SwapMultiPOSNN","SwapMultiPOSJJ","SwapMultiPOSVB","SwapMultiPOSRB","SwapPrefix"],"available_ut":["InsertAdv","AppendIrr","WordCase-lower","WordCase-title","WordCase-upper","Contraction","SwapNamedEnt","Keyboard","MLMSuggestion","SwapNum","Ocr","AddPunc","RevNeg","SpellingError","TwitterType","Typos","SwapSyn-WordEmbedding","SwapAnt-WordNet","SwapSyn-WordNet","Prejudice_Loc_Africa_India_Middle East","Prejudice_Loc_America_Europe_China_Japan","Prejudice_Name_man","Prejudice_Name_woman"]}')},"7c1f":function(e){e.exports=JSON.parse('[{"name":"CTB6","description":"CTB6(Chinese Treebank 6.0) is the latest version produced from the Chinese Treebank project, consisting of 780,000 words (over 1.28 million Chinese characters) that are segmented, part-of-speech tagged and fully bracketed. The data sources include newswire from Xinhua News Agency, articles from Sinorama Magazine, news from the website of the Hong Kong Special Administrative Region and transcripts from various broadcast news programs.","available_transformation_type":["domain","domain_domain"],"dataset_size":2079,"models":[{"model_name":"MCCWS","paper_link":"https://arxiv.org/abs/1906.12035","github_link":"https://github.com/acphile/MCCWS","paper_name":"A Concise Model for Multi-Criteria Chinese Word Segmentation with Transformer Encoder","metric":{"Macro-F1":93.5}},{"model_name":"Sub-CWS","paper_link":"https://arxiv.org/abs/1810.12594","github_link":"https://github.com/jiesutd/SubwordEncoding-CWS","paper_name":"Subword Encoding in Lattice LSTM for Chinese Word Segmentation","metric":{"Macro-F1":96.8}},{"model_name":"GreedyCWS","paper_link":"https://arxiv.org/abs/1704.07047","github_link":"https://github.com/jcyk/greedyCWS","paper_name":"Fast and Accurate Neural Word Segmentation for Chinese","metric":{"Macro-F1":95.1}},{"model_name":"CWS-LSTM","paper_link":"Long Short-Term Memory Neural Networks for Chinese Word Segmentation","github_link":"https://github.com/FudanNLP/CWS_LSTM","paper_name":"https://www.aclweb.org/anthology/D15-1141/","metric":{"Macro-F1":95.2}},{"model_name":"CWS","paper_link":"https://arxiv.org/abs/1606.04300","github_link":"https://github.com/jcyk/CWS","paper_name":"Neural Word Segmentation Learning for Chinese","metric":{"Macro-F1":95}},{"model_name":"CRF","paper_link":"","github_link":"https://github.com/wellecks/cws","paper_name":"","metric":{"Macro-F1":94.1}},{"model_name":"FMM","paper_link":"","github_link":"https://github.com/minixalpha/PyCWS","paper_name":"","metric":{"Macro-F1":86.5}},{"model_name":"BMM","paper_link":"","github_link":"https://github.com/minixalpha/PyCWS","paper_name":"","metric":{"Macro-F1":85.8}}]}]')},"8cbe":function(e){e.exports=JSON.parse('[{"name":"MRPC","description":" The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":1725,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":85.78}},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":87.75}},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":88.73}},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":88.73}},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":89.7}},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.9}},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":88.97}},{"model_name":"albert-large-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.44}},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.68}},{"model_name":"distilbert-base-cased","paper_link":"https://arxiv.org/abs/1910.01108","github_link":"https://github.com/huggingface/transformers/tree/master/examples/distillation","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":84.56}}]},{"name":"QQP","description":"The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":40430,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":90.91}},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/google-research/bert","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":90.98}},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.66}},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.79}},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":91.41}},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/pytorch/fairseq/tree/master/examples/roberta","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":92.03}},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.73}},{"model_name":"albert-large-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.91}},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/google-research/albert","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":92.28}},{"model_name":"distilbert-base-cased","paper_link":"https://arxiv.org/abs/1910.01108","github_link":"https://github.com/huggingface/transformers/tree/master/examples/distillation","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"Accuracy":89.73}}]}]')},9702:function(e){e.exports=JSON.parse('[{"name":"SNLI","description":"The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":10000,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":88.99}},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":89.37}},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":89.45}},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":90.6}},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":89.97}},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.56}},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":88.3}},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":90.19}}]},{"name":"MNLI-m","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":9815,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":84.4}},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":86.18}},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":86.66}},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":89.03}},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.54}},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.6}},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":84.15}},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":89.45}}]},{"name":"MNLI-mm","description":"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut"],"dataset_size":9832,"models":[{"model_name":"bert-base-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":84.42}},{"model_name":"bert-large-uncased","paper_link":"https://arxiv.org/abs/1810.04805","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"Accuracy":86.36}},{"model_name":"xlnet-base-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":86.33}},{"model_name":"xlnet-large-cased","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":88.62}},{"model_name":"roberta-base","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":87.13}},{"model_name":"roberta-large","paper_link":"https://arxiv.org/abs/1907.11692","github_link":"https://github.com/huggingface/transformers","paper_name":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","metric":{"Accuracy":90.12}},{"model_name":"albert-base-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":84.09}},{"model_name":"albert-xxlarge-v2","paper_link":"https://arxiv.org/abs/1909.11942","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations","metric":{"Accuracy":89.89}}]}]')},"9c0d":function(e){e.exports=JSON.parse('[{"name":"SQuAD1.1","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":87599,"models":[{"model_name":"BiDAF","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/galsang/BiDAF-pytorch","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":66.85,"F1":76.6}},{"model_name":"BiDAF+","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":68.1,"F1":77.7}},{"model_name":"DrQA","paper_link":"https://arxiv.org/pdf/1704.00051.pdf","github_link":"https://github.com/hitvoice/DrQA","paper_name":"Reading Wikipedia to Answer Open-Domain Questions","metric":{"EM":69.1,"F1":78.7}},{"model_name":"R-Net","paper_link":"https://www.aclweb.org/anthology/P17-1018.pdf","github_link":"https://github.com/matthew-z/R-net","paper_name":"Gated Self-Matching Networks for Reading Comprehension and Question Answering","metric":{"EM":71,"F1":79.7}},{"model_name":"FusionNet","paper_link":"https://arxiv.org/pdf/1711.07341.pdf","github_link":"https://github.com/momohuang/FusionNet-NLI","paper_name":"FUSIONNET: FUSING VIA FULLY-AWARE ATTENTION WITH APPLICATION TO MACHINE COMPREHENSION","metric":{"EM":71.7,"F1":80.9}},{"model_name":"QANet","paper_link":"https://arxiv.org/pdf/1804.09541.pdf","github_link":"https://github.com/BangLiu/QANet-PyTorch","paper_name":"QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION","metric":{"EM":70.3,"F1":80.2}},{"model_name":"BERT","paper_link":"https://arxiv.org/pdf/1810.04805.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"EM":78.9,"F1":86.9}},{"model_name":"ALBERT","paper_link":"https://arxiv.org/pdf/1909.11942.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","metric":{"EM":83.9,"F1":90.8}},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/pdf/1910.01108.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"EM":76.2,"F1":85.2}},{"model_name":"XLNet","paper_link":"https://arxiv.org/pdf/1906.08237.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"EM":81.1,"F1":89.3}}]},{"name":"SQuAD2.0","description":"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":130319,"models":[{"model_name":"BiDAF","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/galsang/BiDAF-pytorch","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":61.4,"F1":64.5}},{"model_name":"BiDAF+ELMo","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":62.57,"F1":65.38}},{"model_name":"BiDAF+","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":61.75,"F1":65.01}},{"model_name":"BiDAF++ELMo","paper_link":"https://arxiv.org/pdf/1611.01603.pdf","github_link":"https://github.com/sogou/SogouMRCToolkit","paper_name":"Bidirectional Attention Flow for Machine Comprehension","metric":{"EM":64.8,"F1":67.91}},{"model_name":"BERT","paper_link":"https://arxiv.org/pdf/1810.04805.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","metric":{"EM":72.36,"F1":75.82}},{"model_name":"ALBERT","paper_link":"https://arxiv.org/pdf/1909.11942.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","metric":{"EM":79.39,"F1":82.49}},{"model_name":"DistilBERT","paper_link":"https://arxiv.org/pdf/1910.01108.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","metric":{"EM":66.26,"F1":69.68}},{"model_name":"XLNet","paper_link":"https://arxiv.org/pdf/1906.08237.pdf","github_link":"https://github.com/huggingface/transformers","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"EM":81.01,"F1":85}}]}]')},a578:function(e){e.exports=JSON.parse('{"name":"CWS","full_name":"Chinese Word Segmentation","description":"Chinese Word Segmentation is the task of segmenting the correct words in a specific context","available_domain":["SwapName","SwapNum","SwapVerb","SwapContraction","SwapSyn","SwapRedup","SwapGene"],"available_ut":[]}')},a70b:function(e){e.exports=JSON.parse('{"name":"TC","full_name":"Text Classification","description":"Text classification is the task of assigning a sentence or document an appropriate category. The categories depend on the chosen dataset and can range from topics. We only chose large-scale dataset.","available_domain":[],"available_ut":["Typos","Ocr","Keyboard","AddPunc","AddNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","WordCase-lower","WordCase-title","TwitterType"]}')},aa18:function(e){e.exports=JSON.parse('{"name":"COREF","full_name":"Coreference Resolution","description":"Coreference resolution is the task of finding all expressions that refer to the same entity in a text. It is an important step for a lot of higher level NLP tasks that involve natural language understanding such as document summarization, question answering, and information extraction.","available_domain":["RndConcat","RndDelete","RndInsert","RndRepeat","RndReplace","RndShuffle"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","AddNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase-title","WordCase-upper","WordCase-lower","TwitterType"]}')},adbc:function(e){e.exports=JSON.parse('{"name":"NLI","full_name":"Natural Language Inference","description":"Natural language inference is the task of determining whether a \\"hypothesis\\" is true (entailment), false (contradiction), or undetermined (neutral) given a \\"premise\\".","available_domain":["SwapAnt","AddSen","NumWord","Overlap"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RmvPunc","AddNeg","RmvNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType","BackTrans"]}')},b18a:function(e){e.exports=JSON.parse('{"name":"ABSA","full_name":"Aspect-Based Sentiment Analysis","description":"Aspect-based sentiment analysis (ABSA) is an advanced sentiment analysis task that aims to classify the sentiment towards a specific aspect.","available_domain":["RevTgt","RevNon","AddDiff"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RmvPunc","AddNeg","RmvNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType"]}')},b485:function(e,a,t){var n={"./ABSA/task_description.json":"b18a","./COREF/task_description.json":"aa18","./CWS/task_description.json":"a578","./DP/task_description.json":"eb9a","./MRC/task_description.json":"ecf5","./NER/task_description.json":"4ed0","./NLI/task_description.json":"adbc","./POS/task_description.json":"7b5f","./RE/task_description.json":"bcc6","./SA/task_description.json":"fd07","./SM/task_description.json":"7aec","./TC/task_description.json":"a70b"};function i(e){var a=r(e);return t(a)}function r(e){if(!t.o(n,e)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return n[e]}i.keys=function(){return Object.keys(n)},i.resolve=r,e.exports=i,i.id="b485"},bcc6:function(e){e.exports=JSON.parse('{"name":"RE","full_name":"Relation Extraction","description":"Relation extraction aims to classify relation types of given entity pairs in text","available_domain":["SwapEnt-LowFreq","SwapEnt-MultiType","SwapEnt-samEtype","SwapTriplePos-Age","SwapTriplePos-Birth","SwapTriplePos-Employee","InsertClause"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","TwitterType"]}')},ce4f:function(e){e.exports=JSON.parse('[{"name":"IMDB","description":"The IMDb dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score  4 out of 10, and a positive review has a score  7 out of 10. No more than 30 reviews are included per movie. Models are evaluated based on Accuracy.","available_transformation_type":["domain","ut","domain_domain","domain_ut","ut_ut","subpopulation"],"dataset_size":25000,"models":[{"model_name":"XLNET","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":96.17}},{"model_name":"BERT-Large-ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":95.3}},{"model_name":"ULMFIT","paper_link":"https://arxiv.org/abs/1801.06146","github_link":"https://github.com/fastai/fastai/blob/54a9e3cf4fd0fa11fc2453a5389cc9263f6f0d77/examples/ULMFit.ipynb","paper_name":"Universal Language Model Fine-tuning for Text Classification","metric":{"Accuracy":94.6}},{"model_name":"oh-lstm","paper_name":"Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings","metric":{"Accuracy":93.86},"paper_link":"https://arxiv.org/abs/1602.02373","github_link":"http://riejohnson.com/cnn_download.html"},{"model_name":"adv","paper_name":"Adversarial Training Methods for Semi-Supervised Text Classification","metric":{"Accuracy":93.26},"paper_link":"https://arxiv.org/abs/1605.07725","github_link":"https://github.com/tensorflow/models/tree/master/research/adversarial_text"},{"model_name":"ssl","paper_link":"https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_46.pdf","github_link":"","paper_name":"Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function","metric":{"Accuracy":93.23}}]},{"name":"Yelp-Binary","description":"The Yelp Review dataset consists of more than 500,000 Yelp reviews. There is both a binary and a fine-grained (five-class) version of the dataset. Models are evaluated based on error (1 - Accuracy; lower is better).","available_transformation_type":["ut"],"models":[{"model_name":"XLNET","paper_link":"https://arxiv.org/abs/1906.08237","github_link":"https://github.com/zihangdai/xlnet","paper_name":"XLNet: Generalized Autoregressive Pretraining for Language Understanding","metric":{"Accuracy":97.83}},{"model_name":"BERT-Large-ITPT","paper_link":"https://arxiv.org/abs/1905.05583","github_link":"https://github.com/xuyige/BERT4doc-Classification","paper_name":"How to Fine-Tune BERT for Text Classification?","metric":{"Accuracy":98.06}},{"model_name":"ULMFIT","paper_link":"https://arxiv.org/abs/1801.06146","github_link":"https://github.com/fastai/fastai/blob/54a9e3cf4fd0fa11fc2453a5389cc9263f6f0d77/examples/ULMFit.ipynb","paper_name":"Universal Language Model Fine-tuning for Text Classification","metric":{"Accuracy":97.57}},{"model_name":"dpcnn","paper_name":"Deep Pyramid Convolutional Neural Networks for Text Categorization","metric":{"Accuracy":97.34},"paper_link":"https://www.aclweb.org/anthology/P17-1052/","github_link":"http://riejohnson.com/cnn_download.html"},{"model_name":"adv","paper_name":"Adversarial Training Methods for Semi-Supervised Text Classification","metric":{"Accuracy":96.79},"paper_link":"https://arxiv.org/abs/1605.07725","github_link":"https://github.com/tensorflow/models/tree/master/research/adversarial_text"},{"model_name":"ssl","paper_link":"https://www.kdd.org/kdd2018/files/deep-learning-day/DLDay18_paper_46.pdf","github_link":"","paper_name":"Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function","metric":{"Accuracy":97.57}}]}]')},eb9a:function(e){e.exports=JSON.parse('{"name":"DP","full_name":"Dependency Parsing","description":"Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and defines the relationships between head words and words, which modify those heads.","available_domain":["AddSubtree","RemoveSubtree"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","RmvPunc","AddNeg","RmvNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SwapAnt-WordNet","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase","TwitterType"]}')},ecf5:function(e){e.exports=JSON.parse('{"name":"MRC","full_name":"Machine Reading Comprehension","description":"Machine Reading Comprehension is the task is to read and comprehend a given text passage, and then answer questions based on it.","available_domain":["ModifyPos","AddSentDiverse","PerturbAnswer","PerturbQuestion"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","SwapSyn-WordNet","SwapSyn-WordEmbedding","Contraction","Tense","InsertAdv","AppendIrr","WordCase","TwitterType","Prejudice"]}')},fd07:function(e){e.exports=JSON.parse('{"name":"SA","full_name":"Sentiment Analysis","description":"Sentiment analysis is the task of classifying the polarity of a given text.","available_domain":["SwapSpecialEnt-Movie","SwapSpecialEnt-Person","AddSum-Movie","AddSum-Person","DoubleDenial"],"available_ut":["Typos","Ocr","Keyboard","AddPunc","AddNeg","SwapSyn-WordNet","SwapSyn-WordEmbedding","SpellingError","Contraction","Tense","SwapNamedEnt","SwapNum","InsertAdv","MLMSuggestion","AppendIrr","WordCase-upper","WordCase-lower","WordCase-title","TwitterType"]}')}}]);
//# sourceMappingURL=chunk-390fed5d.9eebc7c3.js.map